Какие проблемы есть при общении микросервисов по HTTP ? На самом деле в большинстве ситуаций синхронное общение по HTTP достаточно. Если какому-то сервису нужны данные, то он в любом случае будет обращаться к другому и будет ждать, пока тот не ответит. Но существует ряд ситуаций, когда используется event driven подход, в котором мы хотим сообщать о каких-то эвентах и другие сервисы должны на них реагировать. Тогда столкнемся со следующими проблемами:
1. Если много микросервисов, которым мы хотим сообщить о возникновении какого-то эвента, то нам нужно будет отправлять HTTP запросы каждому из микросервисов.
2. Также во время работы нашего приложения могут подниматься новые микросервисы и мы также должны в случае, если они активны - отправлять их HTTP запросы и сообщать об эвенте.
3. Если какой-то из сервисов стал недоступным, то он никогда не узнает о том, что мы к нему обращались и сообщали о возникновении эвента. Т.е. теряем информацию о возникновении эвента и если какой-то сервис очнулся, то он мог бы обработать все события, которые произошли, пока он был в отключке, но мы эти события никак не сохранили.
![[Pasted image 20250109103216.png]]

Для решения проблем используются брокеры сообщений, например, Apache Kafka. А в целом подход, в котором публикуются эвенты и на них реагируют подписчики - называется Event Driven Architecture. Но нужно понимать, что не всем системам нужен такой подход, поэтому в случаях, где достаточно обычного общения - не нужно его использовать, т.к. это дополнительные накладные расходы. 
![[Pasted image 20250109103945.png]]

### Apache Kafka
![[Pasted image 20250109104333.png]]
![[Pasted image 20250109104451.png]]

Producer создает Event, он попадает в брокер, задачей которого является принять эвент, сохранить его и потом отправить Consumer. Для сохранения эвента используется Topic. Для того, чтобы данные не потерялись - у топика есть его реплики, которые располагаются в других брокерах и в случае, если какой-то брокер упадет, то данные не потеряются, поскольку есть другие брокеры, в которых были реплики данных. Внутри топика есть partititons - очереди, куда как раз и складывается поступающий эвент. Внутри топика может быть несколько partitions.  
![[Pasted image 20250109105055.png]]

### Что такое эвент?
![[Pasted image 20250109105627.png]]
Наименование
![[Pasted image 20250109105645.png]]

### Message vs Event
Message - внутри себя хранит event, а также другие вспомогательные части. На слайде ниже приведены наиболее часто встречающиеся:
![[Pasted image 20250109110115.png]]

### Kafka Topic
Topic имеет уникальное имя. Топик представляет из себя логическое объединение потока данных, которое имеет название и которое используют производители и потребители. Сам топик разделен на части. Это улучшает пропускную способность, т.к. мы можем параллельно разными сервисами читать из разных партиций. Количество партиций, которые будет иметь топик определяется нами при создании топика. При этом мы можем в будущем увеличить количество партиций, но не можем уменьшить! Каждая партиция внутри топика хранится локально на kafka сервере. Партиция представляется в виде строки с ячейками(по сути похоже на массив). Эвенты начинают сохраняться в партиции, начиная с 0. Сами ячейки на слайде ниже - пронумерованы. Эту нумерация называется offset. Также похоже на индекс в массиве, т.е. offset является смещением относительно начала.
![[Pasted image 20250109111324.png]]
Immutable - те эвенты, которые пришли в топик мы никак не можем их редактировать, изменять, удалять. 
Для удаления есть retention time - который по дефолту равен 7 дней. Т.е. в данные топика будут хранится в течение 7 дней, а потом удалятся, но при этом руками явно удалить последней эвент или какой-то эвент в середине партиции - невозможно, как и изменить его. 
### Events Ordering
По дефолту Even-ы, которые поступают в один и тот же топик, если не передан key в message будут распределены kafka, используя load balancer. Т.е. нет какого-то строго порядка сохранения этих эвентов в том порядке, в котором они приходили. А это значит, что и consumer, т.к. обрабатывает эвенты параллельно, может получить эти эвенты в любом порядке! 
В том случае, если порядок важен - например, пользователь меняет в профиле свое имя и делает это несколько раз, то мы хотим сохранить информацию о последнем изменении в БД, но в таком случае, нам важен порядок обработки эвентов, т.к. иначе мы можем сохранить не последнее изменение, а предпоследнее, например. И в том случае, если мы хотим такой порядок, то нужно в message передавать key, на основе которого kafka будет решать в какую партицию сохранить event.
![[Pasted image 20250109112716.png]]

Если мы будем использовать один и тот же message key при публикации эвентов, то такие эвенты будут попадать в один partition. Например, можем использовать userId и в таком случае, эвенты будут обработаны consumer-ом в том порядке, в котором они приходили. Message Key - может быть любым типом, для вычисления партиции вычисляется хэш от набора байт ключа.
![[Pasted image 20250109113252.png]]
### Kafka Broker
Kafka Broker - это программа, в которой запущен процесс kafka. Т.е. по сути это kafka сервер. Внутри брокера располагаются различные топики. Самих брокеров может быть много и они могут быть запущены как локально, так и на удаленных серверах. Множество брокеров = kafka cluster. 
![[Pasted image 20250109114349.png]]
Есть Leader, который принимает и отправляет эвенты, а также есть followers, которые является репликами лидера. Если лидер по какой-либо причине упал, то один из follower становится новым лидером. 

Но если бы была такая схема, когда у нас есть только 1 лидер и только с ним могут взаимодействовать сервисы - то это узкое горлышко, т.к. вся нагрузка идет в него. Поэтому придумали умнее - каждый kafka брокер может быть и лидером и follower-ом.
![[Pasted image 20250109115208.png]]
Когда мы создаем топик и внутри него партиции, то для каждой партиции определяется лидер, т.е. тот kafka broker, который будет работать с данной партицией и записывать в нее данные и отправлять consumer-ам. Другие же брокеры для этой партиции являются фолловерами. Для другой партиции - другой kafka брокер будет лидером. И так для каждой партиции. Таким образом, мы избавились от узкого горлышка, поскольку теперь у нас теперь в работе участвуют все kafka брокеры и нет такого, что один брокер выполняет всю работу. 

А если какой-то kafka брокер упадет, то kafka перераспределит лидером и подписчиков, чтобы система продолжила свою работу, несмотря на падение одного из брокеров. 

### Запуск Kafka сервера
Скачиваем с официального сайта и распаковываем. Для старта раньше использовался подход с Zookeeper, но он deprecated и теперь используется KRaft.
Если зайти в config/kraft, то увидим properties файлы с конфигурациями
![[Pasted image 20250109121143.png]]
controller - в нем содержатся настройки сервера, который является отвественным за выбор лидера и управлением метаданным кластера.
broker - конфигурации брокер сервера, который отвественен за сохранение и отправку эвентов.
server - содержит настройки для сервера, который будет выполнять сразу 2 роли - controllerа и brokera. Т.е. мы можем запустить все так, чтобы один сервер был и брокером и контроллером, но понятно, что в production среде так не делают, т.к. падение такого сервера - автоматический конец всему приложению, поскольку некому будет выбрать нового лидера. 

В bin директории находятся различные скрипты для работы с kafka.
Сначала сгенерируем уникальный ID для kafka cluster.
![[Pasted image 20250109121815.png]]
Затем мы должны подготовить storage директории для запуска kafka clustera. Для этого использует следующую команду, указав после -t ID кластера, который был сгенерирован ранее, а также -c для указания файла с конфигурациями. 
`./bin/kafka-storage.sh format -t NIAIVn7CRMq_eBhcm0Lw9A -c config/kraft/server.properties` 

запускаем kafka server, используя дефолтный файлик с конфигурациями.
`./bin/kafka-server-start.sh config/kraft/server.properties`

Таким образом, имеем запущенный kafka cluster, внутри которого 1 kafka server
### Как запустить несколько kafka серверов внутри kafka cluster
Для кажого kafka сервера создает свой конфигурационный файл.
![[Pasted image 20250109122839.png]]
У каждого kafka сервера должен быть свой уникальный id. Поэтому делаем это поле уникальным для каждого файлика.
![[Pasted image 20250109123146.png]]
Т.к. мы сейчас создаем kafka server, который будет и контроллером и брокером, то тут в настройках в listeners видим, по какому порту будет доступен данный сервер как брокер, а по какому порту будет доступен как controller. Т.к. мы хотим запустить 3 kafka сервера, то должны указать разные порты!
![[Pasted image 20250109123348.png]]

Обновляем во всех файлах настройку, которая указывает какие сервера контроллеры есть, которые будут голосовать и выбирать leader брокер. 
<\node.id>@<\hostname>:<\port>
![[Pasted image 20250109123947.png]]
Изменяем настройку advertised.listeners, которая указывает порт, по которому клиенту смогут обращаться к брокеру. В каждом файлике свой.
![[Pasted image 20250109124238.png]]
Также мы должны сохранять log файлы для каждого брокера в отдельной директории, поэтому для каждого сервера указываем свой subfolder внутри /tmp.
![[Pasted image 20250109124351.png]]

Теперь мы должны подготовить storage directory для kafka cluster. storage directory - это локальное хранилище для каждого kafka servera, в котором kafka cluster будет хранить информацию kafka server-a(метаданные, logs, snapshots). 

![[Pasted image 20250109125232.png]]

Теперь можем запускать командой 
`./bin/kafka-server-start.sh config/kraft/server-1.properties`
И так для каждого конфигурационного файлика. Таким образом, получим 3 запущенных kafka сервера(брокера).

### Остановка Kafka Servera
`./bin/kafka-server-stop.sh` - остановит все kafka сервера, запущенные на данной машине. Это предпочтительный подход остановки, т.к. дает kafka возможность выполнить все необходимые действия для сохранности данных и очистки ресурсов перед остановкой, в то время как ctrl+c убивает процесс и не дает такой возможности.

### Kafka Topic CLI
Удобно использовать для отслеживания состояния. Для работы используем kafka-topics скрипт.
![[Pasted image 20250109134216.png]]
#### Создание топика
![[Pasted image 20250109135705.png]]
- `--partitions 3` - создаем 3 партиции внутри топика. Нет смысла указывать больше, чем будет потребителей.
- `--replication-factor 3` - будет 3 реплики 1 партиции. Если у нас 3 кафка сервера, но на каждом сервере будет по партиции. Указываем не больше, чем у нас есть кафка серверов.
- `bootstrap-server` - указываем инициирующий кафка сервер. При обращении к нему кафка узнает о других серверах в кластере. Но т.к. этот сервер может упасть, то указываем на всякий случай два.
#### Получение информации о топиках
`./kafka-topics.sh --list` - получить список топиков в кластере. 
![[Pasted image 20250109140341.png]]
`./kafka-topics.sh --desribe` - получить более подробную информацию о топиках.
![[Pasted image 20250109140453.png]]
Kafka сохраняет kafka messages в файлах, которые называются logs. Внутри log файл разделен на маленькие кусочки - сегменты. Каждый сегмент имеет максимальный размер, который определяется конфигурацией segment.bytes.
Isr - это какие ноды полностью синхронизированы с leader и в любой момент могут заменить его в случае его падения.
#### Удаление топика
`./kafka-topics.sh --delete --topic topic1 --bootstrap-server localhost:9092`

### Kafka Producer CLI
![[Pasted image 20250109141359.png]]
`./kafka-console-producer.sh --bootstrap-server localhost:9092 --topic topic2` - начать отправлять сообщения в topic2. Причем если мы укажем название НЕСУЩЕСТВУЮЩЕГО ТОПИКА, то по умолчанию кафка создаст такой топик, хоть и ответит с ошибкой, что данного топика нет.
Вот так можно отправлять сообщения:
![[Pasted image 20250109142257.png]]
Как отправить сообщение с ключом?
![[Pasted image 20250109142647.png]]
### Kafka Consumer CLI
![[Pasted image 20250109142820.png]]
Читаем все сообщения из топика:
![[Pasted image 20250109143024.png]]
Для чтения только новых сообщений убираем `--from-beginning` из предыдущей команды. 
### Spring Boot Kafka Producer
![[Pasted image 20250109145810.png]]
Kafka Producer может отправлять сообщения синхронно и асинхронно.
Синхронно: Например, мы хотим получить от брокера подтверждение, что сообщение получено и сохранено! Пользователь сделал заказ, он пришел в Orders Microservice и был передан в брокер, а клиент ждет подтверждения о том, что заказ был успешно создан или нет. Поэтому мы не отвечаем клиенту до тех пор, пока не получим от брокера ответ, причем наш микросервис блокируется, ожидая ответа от сервера.
![[Pasted image 20250109150201.png]]
Асинхронно. Мы отправили сообщение брокеру, при этом не блокируемся в ожидании, а продолжаем работу, причем сообщения о подтверждении мы также может получать от брокера, но главное отличие от синхнонного в том, что мы не блокируемся на время ожидания подтверждения, а работает и просто, когда-нибудь от брокера придет сообщение о подтверждении. 
![[Pasted image 20250109150549.png]]

Например, асинхронный подход может подойти для ситуации, когда пользователь залогинился, то мы хотим отправить UserLoggedInEvent для статистики, но при этом пользователю мы сразу хотим ответить и нам ну нужно дожидаться ответа от Kafka Brokera о том, что он получил этот эвент, т.к. это не очень важный эвент для насшей системы и даже в случае потери ничего страшного не произойдет. 

#### Зависимости
Kafka for Spring.
#### Kafka Producer Configuration Properties
Указываем сериализаторы(инструмент, который будет переводить наш объект в бинарный формат, т.к. при работе с kafka сообщения передаются по сети в бинарном формате, в который мы должны сами перевести сообщение) для ключей и для сообщений. Ключ - это строка, поэтому используем StringSerializer(но могли и другие сериализатор использовать в зависимости от того, какого типа мы хотим иметь ключи в нашем приложении) и value - JsonSerializer, потому что хотим передавать в сообщениях JSON. 
bootstrap-servers - начальная точка входа для подключения producera к kafka cluster-у. Т.е. указываем какой-либо из kafka серверов, которые работают в kafka кластере.
![[Pasted image 20250109152934.png]]
#### Создание топика при запуске микросервиса
```java
@Configuration  
public class KafkaConfig {  
  
    @Bean  
    NewTopic createTopic() {  
        return TopicBuilder.name("product-created-events-topic")  
                .partitions(3)  
                .replicas(3)  
                .configs(Map.of("min.insync.replicas", "2"))  
                .build();  
    }  
}
```
min.insync.replicas - определяет сколько insync реплик должны подтвердить, что записали данные. Если заданное количество не подтвердит, то продьюсеру вернется ошибка.

После этого можем запускать приложение(только kafka кластер должен быть запущен) и увидим, что создастя наш топик.

#### Отправка сообщения асинхронно
```java
@Service  
public class ProductServiceImpl implements ProductService {  
  
    private final KafkaTemplate<String, ProductCreatedEvent> kafkaTemplate;  
    private final Logger LOGGER = LoggerFactory.getLogger(this.getClass());  
  
    public ProductServiceImpl(KafkaTemplate kafkaTemplate) {  
        this.kafkaTemplate = kafkaTemplate;  
    }  
  
    @Override  
    public String createProduct(CreateProductRestModel productRestModel) {  
        String productId = UUID.randomUUID().toString();  
        // TODO: Persist Product Details into database table before publishing an Event  
  
        ProductCreatedEvent productCreatedEvent = new ProductCreatedEvent(  
                productId,  
                productRestModel.getTitle(),  
                productRestModel.getPrice(),  
                productRestModel.getQuantity()  
        );  
  
  
        CompletableFuture<SendResult<String, ProductCreatedEvent>> future =  
                kafkaTemplate.send("product-created-events-topic", productId, productCreatedEvent);  
  
        future.whenComplete((result, exception) -> {  
            if(exception != null) {  
                LOGGER.error("Failed to send message: " + exception.getMessage());  
            } else {  
                LOGGER.info("Message was sent successfully" + result.getRecordMetadata());  
            }  
        });  
  
        // блокирует текущий поток до тех пор, пока future не будет завершен.  
        // так мы получим синхронное выполнение(но есть более удобный вариант)   
        //future.join();  
          
        return productId;  
    }  
}
```
#### Отправка сообщений синхронно(т.е. будет дожидаться подтверждения от kafka о доставке)
```java
@Service  
public class ProductServiceImpl implements ProductService {  
  
    private final KafkaTemplate<String, ProductCreatedEvent> kafkaTemplate;  
    private final Logger LOGGER = LoggerFactory.getLogger(this.getClass());  
  
    public ProductServiceImpl(KafkaTemplate kafkaTemplate) {  
        this.kafkaTemplate = kafkaTemplate;  
    }  
  
    @Override  
    public String createProduct(CreateProductRestModel productRestModel) throws Exception {  
        String productId = UUID.randomUUID().toString();  
        // TODO: Persist Product Details into database table before publishing an Event  
  
        ProductCreatedEvent productCreatedEvent = new ProductCreatedEvent(  
                productId,  
                productRestModel.getTitle(),  
                productRestModel.getPrice(),  
                productRestModel.getQuantity()  
        );  
  
        SendResult<String, ProductCreatedEvent> result =  
                kafkaTemplate.send("product-created-events-topic", productId, productCreatedEvent).get();  
  
        return productId;  
    }  
}
```
Из SendResult, который к нам возвращается при вызове метода send мы можем получить различную метаинформацию:
![[Pasted image 20250109172433.png]]
![[Pasted image 20250109172538.png]]

### Kafka Producer Acknowledgement
По дефолту мы имеем следующее поведение:
Kafka Producer отправил в брокер сообщение и дальше дождался от него подтверждения о том, что сообщение сохранено, но т.к. мы работаем только с лидером, то подтверждение по дефолту также приходит только от него. Но, что, если лидер подтвердил, что сохранил сообщение, а потом упал, не успел поделиться этим сообщением с другими репликами. Тогда мы потеряли сообщение. 
![[Pasted image 20250110073608.png]]
В этом разделе мы настроем kafka producer так, чтобы producer ожидал ack больше, чем от одного брокера.
Да, приложение будет работать медленнее, т.к. теперь будет ожидаться ack от лидера и фолловеров, но если у нас критическая нужда, чтобы сообщение ни в коем случае не потерялось, то мы можем на такое пойти.
![[Pasted image 20250110074207.png]]
Важно заметить, что ack мы ожидаем только от in-sync реплик, а не от всех followers(видимо, есть еще и не in-sync реплики). Мы можем настроить
- min.insync.replicas - сколько ack от in-sync реплик мы минимум ожидаем. Т.е. если хотя бы 2 реплики подтвердили, что сохранили сообщение, то мы уже можем не дожидаться подтверждений от других реплик и продолжить свою работу. 
- replication-factor - это сколько in-sync реплик должно быть, настраивается при создании топика.
![[Pasted image 20250110074443.png]]

### Retries
Пусть мы настроили параметр min.insync.replicas=3. Т.е. теперь мы ожидаем, что нам придет подтверждение в producer от трех брокеров о получении сообщения. Но, что, если один брокер упал, а у нас всего 3 реплики и получается, что 3-е подтверждение не пришло. В таком случае, по дефолту происходит следующее - kafka producer использует механизм ретраев. Он отправляет сообщение в in-sync реплику, пока:
1. Получит подтверждение, что сообщение сохранено
2. Превзойдет заданное число попыток отправки
3. Пройдет 2 минуты и подтверждение так и не будет получено.
![[Pasted image 20250110075000.png]]
Варианты ответов от брокера. И в случае ошибки есть два варианта:
- Non-Retryable Error - ошибка, которую невозможно решить ретраем. Например, сообщение слишком больше и сколько не пытайся его отправлять - все равно ничего не изменится.
- Retryable Error - ошибка, которую можно потенциально решить повторной отправкой. Например, когда на текущий момент недостаточно живых in-sync реплик или сообщение не доставилось из-за проблем с сетью.   
Но это не мы решаем, какой тип ошибки произошел - этим занимается kafka producer. Если он решил, что это retryable error, то выполняет send operation основываясь на конфигурации, которую мы можем задать. 
![[Pasted image 20250110075541.png]]
Конфигурация retries:
![[Pasted image 20250110080649.png]]
Но есть рекомендованные конфигурации, вместо тех, что мы написали выше. Если за delivery.timeput.ms ack не был получен, то kafka producer выбросит исключение. 
![[Pasted image 20250110080848.png]]

Чтобы настроить Kafka Producer ждать подтверждения от всех in-sync реплик в конфигурационном файле указываем следующее:
![[Pasted image 20250110081448.png]]
Задать минимальное количество in-sync реплик, которые должны ответить:
1. При создании топика:
   `.configs(Map.of("min.insync.replicas", "2"))`)
   или если через cli создавали:
   `./bin/kafka-topics.sh --create --topic insync-topic --partitions 3 --relication-factor 3 --bootstrap-server localhost:9092 --config min.insync.replicas=2`
2. Если топик уже существует:
   ![[Pasted image 20250110082833.png]]

После того, как мы сделали такие настройки, где нам требуется, чтобы подтверждение о получении сообщения отправили как минимум 2 реплики, то если у нас 3 kafka сервера, и мы остановим 2, т.е. никак не сможем получить подтверждение от 2 других реплик, то после отправки запроса мы будем ждать 2 минуты(это как раз delivery.timeout.ms настройка) и по итогу получим следующую ошибку:
![[Pasted image 20250110083538.png]]
Причем consumer не получит такое сообщение! Т.е. даже несмотря на то, что в лидер сообщение пришло, но от двух других реплик подтверждения не было, то будет считаться, что отправка такого сообщения не завершилась успешно, т.к. минимального числа подтверждений о получении не было!

С помощью application.properties файла мы можем настраивать поведение kafka producera.
![[Pasted image 20250110091507.png]]
Но также мы можем настраивать Kafka Producer, используя Java код. Создаем Map, в котором задаем конфигурации для нашего Kafka Producer
![[Pasted image 20250110091953.png]]
И создаем Bean для ProducerFactory, который будет заниматься созданием Producera.
![[Pasted image 20250110092153.png]]
И создаем теперь KafkaTemplate, который мы и использовали для отправки эвентов. 
![[Pasted image 20250110092325.png]]
KafkaTemplate - это обертка над Kafka Producer и упрощает отправку сообщений. А Kafka Producer - это просто low level api, которая как раз занимается отправкой.

Java конфигурация выше по приоритету, чем application.properties конфигурация.

Зачем нужна java конфигурация? 
1. Мы можем через if else задавать динамические настройки для producera
2. Наш микросервис может работать одновременно с несколькими топиками и отправлять несколько эвентов и может быть ситуация, где нам для разных топиков нужны разные настройки producera. В таком случае, нужно будет создавать несколько RestTemplate объектов и настраивать их по-разному в зависимости от эвентов, которые они будут посылать. 

### Идемпотентный Kafka Producer
Мы можем получить такой сценарий:
![[Pasted image 20250110093759.png]]
Т.е. ack не дошло до producera(т.к. передача по сети и из-за сбоя ack мог не дойти), то выполнился механизм retry и сообщение отправилось еще раз. В таком случае, у нас в kafka topic находится два идентичных сообщения. И в каких-то приложениях это может оказаться проблемой - например, если это было создание заказа в интернет-магазине, то один и тот же заказ будет создан дважды. Поэтому может потребоваться создание идемпотентного producera.
![[Pasted image 20250110094010.png]]
После включение идемпотентности producera будем иметь следующую логику работы:
![[Pasted image 20250110094134.png]]
Вот так включается данный механизм(но уже такой механизм включен по дефолту)
![[Pasted image 20250110094219.png]]
Но даже несмотря на то, что по дефолту данная настройка включена, то ее нужно все равно явно включать, поскольку мы можем случайно ее выключить, причем неявно, а используя конфликтующие значения для других параметров, которые выключат эту настройку. На скрине ниже приведены настройки, которые нужно использовать для того, чтобы идемпотентность работала.
![[Pasted image 20250110094505.png]]
max.in.flight.requestrs.per.connection - это сколько максимум отправок сообщений(как батчем так и по отдельности) может отправить producer, не дожидаясь подтверждения. Если сделать больше, то появляется риск потерять сообщения, а также из-за ошибок и ретраев порядок сообщений также может стать неверным. Поэтому указываем <= 5 число для этой настройки.

Если мы поставим enable.idempotence=true, а другие значение будут конфликтными для такой настройки, то получим ConfigException. 

С помощью идемпотентности producera мы получаем гарантию [[Exactly once]].

### Kafka Consumer
Когда мы читаем сообщения из какого-то топика, то нет гарантии, в каком порядке мы прочитаем сообщения, если они располагаются в разных партишенах. Т.е. нет никакой гарантии, что мы сначала прочитаем все сообщения из партишена 1, а потом только все сообщения из партишена 2.

Чтение сообщения - не удаляет сообщение. Оно продолжает хранится в kafka и по дефолту оно хранится 7 дней.
Необходимые зависимости
![[Pasted image 20250110102924.png]]
Настройки для Consumer application.properties:
```properties
spring.kafka.consumer.bootstrap-servers=localhost:9092,localhost:9094  
spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer  
spring.kafka.consumer.value-deserializer=org.springframework.kafka.support.serializer.JsonDeserializer  
spring.kafka.consumer.group-id=product-created-events  
spring.kafka.consumer.properties.spring.json.trusted.packages=*
```
trusted.packages - это пакеты, где располагаются классы, в которые будут десериализовываться JSON данные из kafka. Это нужно для безопасности, чтобы указать Kafka Consumer-у, в какие классы разрешено десереализовывать данные из Kafka. This property specifies the packages that are considered safe for JSON message deserialization. It's a security measure to prevent the application from deserializing objects from unknown or untrusted sources, thus mitigating potential security risks.
![[Pasted image 20250110140512.png]]

Определим метод, который будет вызываться, когда в указанный топик придет сообщение.
![[Pasted image 20250110133944.png]]
Если мы внутри класса хотим слушать множество типов эвентов, то тогда делаем вот так, чтобы не писать над каждым методом @KafkaListener:
Тип эвента, который мы слушаем указываем в параметрах метода.
![[Pasted image 20250110135734.png]]
#### Конфигурация Kafka Consumer, используя Java Config
![[Pasted image 20250110140836.png]]
И после задания конфигурации создаем бин:
![[Pasted image 20250110141156.png]]
### Handle Deserializer Errors
Если не обрабатывать такую ошибку, то наш микросервис будет в бесконечном цикле ошибок, поскольку он будет каждый раз пытаться прочитать сообщение, которое вызвало ошибку.
![[Pasted image 20250110163339.png]]
Добавляем две строки: 
1. В качестве десериализатора указываем ErrorHandlingDeserializer - это обертка поверх десереализатора, которая обрабатывает ошибки десериализации. Но вокруг какого десериализатора оборачиывается ErrorHandlingDeserializer? За это отвечает вторая  строка.
2. Указываем какой десереализатор будет использовать ErrorHandlingDeserializer
![[Pasted image 20250110164302.png]]
После этого не будет зацикленности, т.е. после того, как произошла ошибки десериализации какого-то сообщения, то повторных чтений этого же сообщения больше не будет.
### Dead Letter Topic
То, как мы решили проблемы с ошибкой десереализации - это только часть решения, поскольку мы просто игнорируем сообщения, с которыми произошла какая-то проблема. Вместо игнорирования мы можем посылать такие сообщения в DLT - Dead Letter Topic и потом либо проанализировать эти сообщения и понять, что пошло не так, либо даже используя другой микросервис, который читает сообщения из этого топика и что-то делает.
![[Pasted image 20250110165149.png]]

По дефолту DLT топик имеет следующее название: <название топика, в который пришло сообщение, которое десереализуется с ошибкой>.DLT
![[Pasted image 20250110165404.png]]
Реализация:
![[Pasted image 20250110170224.png]]
Но, т.к. тут используется kafkaTemplate, то нам нужно ее также настроить для отправки сообщений в DLT:
![[Pasted image 20250110170812.png]]
KafkaTemplate зависит от ProducerFactory, т.к. ProducerFactory умеет создавать Producer-ов.
