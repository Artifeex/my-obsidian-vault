Основана на хэш-таблицах, реализует интерфейс Map. 
![[Pasted image 20240925170055.png]]
HashMap хранит пары - ключ, значение.
Под капотом в HashMap есть массив bucket-ов. В них сохраняются Node(узлы). В каждой ноде хранится ключ и значение. По дефолту создается 16 бакетов. Со временем этот массив увеличивается, с увеличением числа элементов в HashMap. А это влечет за собой то, что нужно будет потратить ресурсы на перераспределение сохраненных элементов, т.к. формула по вычислению ячейки зависит от размера массива бакетов. Такое перераспределение происходит, когда превышается loadFactor - процент заполненности.

### Процесс вставки в HashMap
При вызове метода put. У ключа вызывается вызов метода hashCode(), чтобы получить hashCode - это какое-то большое число. И нам необходимо положить этот ключ в какой-то один из бакетов. Для этого мы берем хеш-код и производим операцию побитового И. Каждый бакет представляет собой LinkedList(там его кастомная реализация). И в этом бакете хранятся все элементы, у которых 
hashCode && N дал одно и тоже значение. После этого мы должны проверить, что внутри этого списка нет элемента с таким же ключом(т.е. производится дублирующая вставка). Для этого мы проверяем с помощью equals значения ключа(НЕ ХЭШ, А ИМЕННО ЗНАЧЕНИЕ КЛЮЧА). Если равны, то, значит, что это дублирующая вставка, поэтому мы просто обновляем значение по этому элементу. А если не была найдено ни одного ключа с таким же значением, то происходит вставка. 

Данная реализация не дает гарантий относительно порядка элементов с течением времени. Разрешение коллизий осуществляется с помощью метода цепочек.


Побитовое "И" используется, потому что это быстро. 

Также стоит заметить, что LinkedList может со временем для оптимизации перестроиться в красно-черное дерево, если будет 8 элементов в LinkedList. И за счет этого мы можем найти нужный ключ, который нам надо за log(N).

### Асимптоты 
- Вставка - O(1) (но в худшем случае может быть и O(N), если все элементы попадут в один bucket)
- Поиск - O(1) (но в худшем случае может быть и O(N), если все элементы попадут в один bucket)
- Удаление - O(1) но в худшем случае может быть и O(N), если все элементы попадут в один bucket
- Среднее время работы можно оценить как O(1 + a), где a - коэффициент загрузки.

### Что внутри HashMap
Новоявленный объект hashmap, содержит ряд свойств:
- table — Массив типа Entry[], который является хранилищем ссылок на списки (цепочки) значений; Т.е. внутри Entry будет храниться ссылка на первый элемент цепочки.
    
- loadFactor — Коэффициент загрузки. Значение по умолчанию 0.75 является хорошим компромиссом между временем доступа и объемом хранимых данных;
    
- threshold — Предельное количество элементов, при достижении которого, размер хэш-таблицы увеличивается вдвое. Рассчитывается по формуле (capacity * loadFactor);
    
- size — Количество элементов HashMap-а;
    

Оказывается, что помимо получения hashCode у ключа, его hashCode передается в функцию hash, в которой происходят побитовые сдвиги. Это нужно для того, чтобы гарантировать ограниченное число коллизий при дефолтном значении loadFactor.

Так считается loadFactor. Когда он превысится, то hashMap увеличится. 
loadFactor = number of entries / current capacity
Например, если мы зададим loadFactor = 0.75, а currentCapacity по дефолту 16, то 0.75 * 16  = 12, а это значит, что когда в таблице будет 12 элементов, то произойдет увеличение hashMap.

Порог рехеширования(увеличения hashMap) - это как раз число 12, которое получилось по формуле loadFactor * currentCapacity.

### Процесс вставки в HashMap(более подробный)
![[Pasted image 20241022144039.png]]
![[Pasted image 20241022144049.png]]
![[Pasted image 20241022144108.png]]

Стоит обратить внимание на несколько моментов, первый - это то, что есть функция hash(hashCode), в которую передается hashCode объекта, она обеспечивает то, что при заданном loadFactor будет ограниченное число коллизий(для loadFactor = 0.75 число коллизий не будет превышать 8).

Когда нашли нужный bucket, то при дальнейшем сравнении сначала сравнивается hash, а уже потом, если hash совпали, то сравнивается и значение ключей.

При коллизии новый элемент в bucket добавляется в начало цепочки.

### Что будет, если переданный ключ = null?
hash у того ключа мы посчитать не может и оказывается в hashMap вызывается метод putForNullKey(value). Все элементы с null ключами помещаются в table[0] (в bucket с индексом 0). Дальше внутри этого bucket ищется элемент в цепочке с ключом = null. Если такого не нашлось, то добавляем в цепочку такой элемент, а если уже был элемент с null ключом, то перезаписываем его значение.

### Если мы превысили threshold, то происходит вызов метода resize и transfer(перераспределяет элементы по HashMap с учетом нового размера)

### Удаление элементов - интересно то, что если добавить сразу много элементов и пройзодет множество расширений, то обратно HashMap не сузится. Этим страдает и ArrayList, но в нем есть метод trimToSize(), которым можно ненужную память освободить

### Итераторы в HashMap
HashMap имеет встроенные итераторы, такие, что вы можете получить список всех ключей keySet(), всех значений values() или же все пары ключ/значение entrySet(). Ниже представлены некоторые варианты для перебора элементов:

```java
// 1.  Set<Entry>
for (Map.Entry<String, String> entry: hashmap.entrySet())  
    System.out.println(entry.getKey() + " = " + entry.getValue());  
  
// 2.  Set ключей
for (String key: hashmap.keySet())  
    System.out.println(hashmap.get(key));  
  
// 3.  
Iterator<Map.Entry<String, String>> itr = hashmap.entrySet().iterator();  
while (itr.hasNext())  
    System.out.println(itr.next());

//4. Можно получить и список всех значений List<String>
hashmap.values()
```

Стоит помнить, что если в ходе работы итератора HashMap был изменен (без использования собственным методов итератора), то результат перебора элементов будет непредсказуемым.

Также существует еще и [[LinkedHashMap]]

### Потокобезопасные варианты HashMap
1. Hashtable - старая реализация потокобезопасной коллекции HashMap, ее сейчас лучше не использовать
2. Collections.synchronizedMap - блокируется сразу вся Map. Т.е. если один поток зашел в нее, то другие уже не смогут.
3. [[ConcurrentHashMap]] - лучше всего использовать ее. Отличается от Collections.synchronizedMap тем, что она не блокирует сразу все Map. Она блокирует только bucket с которым работает поток, другие потоки смогут без проблем использовать другие бакеты. 

