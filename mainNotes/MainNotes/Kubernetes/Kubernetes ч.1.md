### Какой была жизнь до Kubernetes?
Мы хотим развернуть наш микросервис. Что нам для этого нужно? Нам нужна машина, на которой все это разворачивать. На эту машину нужно передать наш код, потом нам нужна БД, нужен файл для конфигурации. И все это нужно делать руками. Потом мы решили создать еще инстансы нашего микросервиса - а это снова повторяется тот процесс, когда нужно руками все настроить. Потом т.к. у нас несколько инстансов, то нам нужен какой-то балансировщик нагрузки, который будет распределять нагрузку между инстансами - вот уже и nginx появился. Ну и в конце концов получим такую картину:
![[Pasted image 20241210162101.png]]
И это мы создали тестовый контур, а нам нужно еще повторить все то же самое на product контуре.
![[Pasted image 20241210162629.png]]
### Kubernetes Cluster
Kubernetes Cluster - это как компьютерный кластер - такая система, которая состоит из нескольких компьютеров. Только такие компьютеры в Kubernetes кластере называются нодами.
![[Pasted image 20241210162801.png]]
Два вида Node:
- Worker -  на этих компьютерах будет запускаться приложение, которое мы хотим хостить на них.
- Master - нода, которая управляет жизненным циклом самого kubernetesa.

Как развернуть приложение внутри worker node?
Напрямую мы в нем не можем поднять наш контейнер. В Kubernetes есть дополнительная абстракция - pod. И вот внутри нее мы уже можем развернуть наш контейнер. У каждого пода внутри кластера есть IP адрес. Причем этот IP адрес именно внутри Kubernetes кластера. Извне если по этому IP адресу обратиться, то мы ничего не получим. А вот если внутри этого кластера какое-то приложение обратится по этому адресу, то оно обратится к поду. Обычно принято, что в одном поде - одно приложение. Но в некоторых случаях в одном поде может быть и два приложения. Пример такого случая: первое приложение может работать и писать логи, а второе приложение агрегирует эти логи и потом куда-то отсылает(например, в grafana есть alloy agent, который как раз занимается тем, что собирает логи и отправляет их в grafana loki - агрегирующий контейнер логов). И вот как раз alloy и приложение, которое генерит логи могут находиться внутри одной поды. Если обобщить, то можно развернуть сразу два контейнера в одной поде в том случае, если одно из приложений расширяет функционал другого.
![[Pasted image 20241210163250.png]]

### Добавим детализации в схему
![[Pasted image 20241210163927.png]]
На каждой Node стоит 
- Kubelet, который контролирует жизненный  ноды и самих приложений на этой ноде.
- k-proxy - отвечает за сетевое взаимодействие с API сервером на worker нодах.

### Как работать с Kubernetes?
Основная команда kubectl:
- kubectl get nodes - получить информацию по нодам, запущенным на компьютере.
  ![[Pasted image 20241210164832.png]]
- kubectl get pods - посмотреть поды.
  ![[Pasted image 20241210164919.png]]
  Видим, что тут вернуло что не нашло ресурсов в default namespace. Значит, есть namespaces. Это такие логические разделения. Под каждое приложение можем заводить свои неймспейсы.
- kubectl get namespaces - получить все неймспейсы. Все, что представлены ниже - это стандартные, которые создает для своих нужд кубер.
  ![[Pasted image 20241210165231.png]]
  - kubectl get pods -n <неймспейс>:
  ![[Pasted image 20241210165344.png]]

Работать с кубером мы можем не только через командную строку. Но еще и через dashboard:
![[Pasted image 20241210165514.png]]

### Как развернуть наше приложение в кубере?
Можем представить себе кубер как набор строительных блоков - каждый строительный блок - это и есть наше приложение. Строительные блоки бывают разные. Если привести аналогию с лего, то строительным блоком может быть деталька 5 на 8 или 10 на 5 и т.д. И вот из таких деталек строится весь кубер кластер.

Такие блоки описываются с помощью yml файла
![[Pasted image 20241210165829.png]]
- apiVersion - версия, которая будет использоваться для построения блока.
- kind - как раз тип строительного блока.
- metadata - информация, которая используется для того, чтобы мы могли различать наши приложения внутри кластера. Т.е. какая-то информация о том приложении, которое запущено внутри строительного блока
- spec - требуемое состояние объекта.

Опишем проект, который мы хотим развернуть:
![[Pasted image 20241210170107.png]]

Пусть есть микросервис notes, который имеет два метода. POST - для создания заметки(передается текст и attachment - это файл в кодировке base64, который можно прикрепить к заметки) и GET метод, с помощью которого можно получить заметки. Предполагаем пока, что БД нет. Что нужно сделать, чтобы развернуть такое приложение в кубере?
![[Pasted image 20241210170355.png]]

Сначала нам нужен pod, в котором уже будет работать наше контейнерезированное приложение.

Мы знаем, что для создания объектов в kubernetes нужно использовать yml файлы. Напишем такой:
![[Pasted image 20241210171424.png]]
Отлично, у нас есть описанный блок. Как нам его применить ?
apply - хотим применить такой строительный блок -f - файлик и путь до файлика:
![[Pasted image 20241210171527.png]]
Под создан. с помощью kubectl describe pods notes-pod можем получить информацию о нем:
![[Pasted image 20241210171635.png]]
Из интересного мы видим IP адрес, по которому могло бы обратиться другое приложение, развернутое в этом кластере. Также видим, что pod находится в дефолтном namespace.

Посмотреть логи - kubectl logs notes-pod
![[Pasted image 20241210171855.png]]

Либо можем посмотреть все через нашу dashboard.

Как удалить под?
kubectl delete pods notes-pod.

Если мы изменим наше приложени, потом его пересоберем и снова запустим, то на самом деле приложение в кубере не обновится, т.к. мы НЕ изменили в image контейнера внутри spec версию v1. А т.к. кубер это видит, то он просто использует кэш и не будет пересоздавать обновленный под. 

Как победить эту проблему? Самый очевидный вариант - изменить версию у image. Но это не всегда возможно. Второй вариант - использовать imagePullPolicy:

### imagePullPolicy
![[Pasted image 20241210172836.png]]
- IfNotPresent - образ скачивается в том случае, если он не сохранен локально. Это то, что произошло, когда мы изменили приложение, но изменения не отобразились внутри Kubernetes. Такое поведение по умолчанию. Kubernetes посмотрел, увидел, что у него локально есть такой образ и не стал его качать.
- Always - всегда будет скачивать из внешнего репозитория образы.
- Never - будет использовать только локальные образы. А если не найдет в кэше образ, то выбросит ошибку.
Добавим imagePolicy в наш под:
![[Pasted image 20241210173218.png]]

Хорошо, мы развернули. Но как теперь протестировать и обратиться к этому приложению? На самом деле при разработке есть команда, которая позволяет открыть временный доступ к поде из внешнего мира.

Вот так мы можем пробросить порт нашего приложение в открытый мир(так можно делать только при разработке, на проде так делать не надо :) )
kuberctl port-forward notes-pod 111111:8080
Теперь мы можем обратиться по такому порту 111111.

Теперь добавим в наш проект переменные окружения. Но как их задать в kubernetes?
![[Pasted image 20241212093616.png]]

### Конфигурация приложения
![[Pasted image 20241212093744.png]]
![[Pasted image 20241212093802.png]]
Эти переменные окружения мы можем задать внутри строительного блока(poda). Но это как-то неправильно, т.к. мы же заказчику передаем pod нашего приложения, а у него своя БД может быть на продакшене. Или если мы хотим предоставлять наше приложение сразу нескольким заказчикам, тогда как поступить? Создавать для каждого приложения такой pod файл как-то глупо.
![[Pasted image 20241212094147.png]]

Хотелось бы вынести эти настройки в отдельное место.
### ConfigMap
Она позволяет отделить описание poda, от его конфигурации. И теперь заказчику мы будем поставлять настройки пода, а вот ConfigMap у всех будет разная.
![[Pasted image 20241212094430.png]]
ConfigMap - это определенный тип строительного блока. Поэтому создаем для него отдельный файлик. Поэтому kind - ConfigMap, а в data мы как раз передаем значения, которые хотим сохранить как пары: ключ-значение.
![[Pasted image 20241212094658.png]]
![[Pasted image 20241212095117.png]]
И теперь заказчику мы передаем notes-pod.yml, а вот notes-db-config он уже сам может написать и вставить нужные значения для своей продакшен БД.

Но это все еще неоптимально, поскольку небезопасно. Т.к. если мы будем передавать файл и какой-то нехороший человек посмотрит в файлик, то увидит пароли к нашей бд.

### Secret
Более оптимальный способ для хранения секретной информации. Похож на ConfigMap, но разница в том, что значения кодируются в base64 и даже если кто-то случайно взглянет на наш файлик, когда будет проходить мимо монитора, то он не сможет запомнить пароль.
![[Pasted image 20241212095812.png]]
Мы в одном файле можем описывать сразу несколько блоков. Для этого их нужно разделить ---
![[Pasted image 20241212100037.png]]

Создаем блок Secret:
![[Pasted image 20241212100254.png]]
В целом все также, как и в ConfigMap, также в блоке data указываем пары ключ значение. Но дополнительно еще указывается type - тип секрета. Самый популярный - это Opaque(непрозрачный). И вот где мы написали пароль, то там на самом деле написано postrgres, но в формате base64.

Еще плюс secret в том, что kuber не хранит их в постоянной памяти, а хранит в оперативной внутри master node.

secretKeyRef - для ссылки на ключ из secret, чтобы задать значение внутри нашего пода.
![[Pasted image 20241212100727.png]]

### Пришло время выкатывать в prod
Как нам достучаться до нашего приложения извне? Мы настроили до этого port forwarding. Но так можно делать только при разработке.
Для решения проблемы существует отдельный компонент - Service.
![[Pasted image 20241212101701.png]]
### Виды сервисов
#### NodePort
Мы создаем блок Service c типом NodePort и указываем порт, по которому к этому сервису смогут обращаться извне. Но как этот сервис узнает о подах, на которые нужно будет переадресовывать запросы? Для этого для подов будут задаваться label и в сервисе указывается, чтобы он хранил внутренние ip-шники у себя в табличке. 
![[Pasted image 20241212102603.png]]

Причем кубернетес еще и следит за тем, чтобы поды были активными. Т.е. если какая-то пода умрет из-за того, что нода, на которой работает сервис померла, то он обновит табличку в сервисе:
![[Pasted image 20241212102612.png]]
И в обратную сторону тоже работает. Если новый под поднялся, то после того, как он будет полностью готов принимать запросы, то он добавится в табличку сервиса.

Реализация.
Вешаем ярлычок на нашей поде:
![[Pasted image 20241212102831.png]]
Причем создадим два пода(однако нам пока приходится просто дублировать код - это задел на будущее).
В kuberntes dashboard вот два запущенных пода:
![[Pasted image 20241212103310.png]]
Создаем блок service. В нем указываем в selector, что выбирай только такие блоки, у которых есть label app со значением notes.
port: 80 - это сам service работай на таком порту. А 
targetPort - это порт самого приложения(мы задали внутри пода имя для порта http. На скрине ниже увидишь)
nodePort - наружний порт нашего сервиса.
![[Pasted image 20241212105421.png]]

Задаем имя для порта в нашей поде основного приложения:
![[Pasted image 20241212105812.png]]

С помощью kubectl describe service notes-service получим описание сервиса:
![[Pasted image 20241212110230.png]]
И когда делаем запрос, то мы в качестве хоста должны указать адрес любой рабочей ноды, т.к. у нас кубер запущен локально, то адрес у них localhost, а вот порт мы указываем тот, что указывали у service и тогда этот запрос пойдет как раз в сервис, а потом от сервиса будут перенаправлен в поду с label app=notes
![[Pasted image 20241212110406.png]]

Ну и круто то, что kubernetes сам следит за состоянием подов и в случае чего исключает их из сервисов.

### Теперь переделаем нашу систему
До этого мы файлы и заметки хранили в одной БД, что плохо, поскольку файлы могут быть здоровыми и сильно нагружать БД. Поэтому для файлов мы создали отдельную БД(что тоже неправильно, нужно использовать какое-нибудь minio) и сервис attachments, который загружает файл в свою БД с файлами.

Сохранение заметок теперь происходит следующим образом:
![[Pasted image 20241212111203.png]]

Получение заметок:
![[Pasted image 20241212111314.png]]
Но теперь нам нужно, чтобы два разных сервиса общались друг с другом. Как нам это сделать?
Для этого нам нужно использовать сервис ClusterIp.

### ClusterIp
Он предоставляет доступ к списку подов с определенным лейблом, но только уже для внутреннего пользования внутри кубер кластера.
Мы шлем запрос на сервис, а он уже распределяют запрос на поды. В адресе запроса мы указываем название сервиса:
![[Pasted image 20241212111902.png]]
Создаем Pod для сервиса attachments, а также указываем у него label. Плюс создаем еще 2 блока для этого пода, чтобы он подключился к своей БД(ConfigMap и Secret).
![[Pasted image 20241212112043.png]]
И теперь пишем блок сервиса, который будет использоваться для внутреннего общения внутри сети. И мы можем заметить, что для Service мы не прописали type: ClusterIP, как мы это делали для сервиса для внешнего доступа type: NodePort.
![[Pasted image 20241212112445.png]]

Для общения используем FeignClient. в котором в url указываем название нашего сервиса
![[Pasted image 20241212114251.png]]
Вот такой url будет в feign client для общения.
![[Pasted image 20241212114535.png]]

После этого мы научились выстраивать общение внутри кубернетес. И самое прикольное, что attachments он извне недоступен, но внутри кубернетеса с ним можно общаться.

#### ExternalName
Еще один полезный сервис. Допустим мы хотим, чтобы у нас было 2 БД. Одна для prod контура, а другая для dev. И ip БД будем хранить внутри этого сервиса. 
![[Pasted image 20241212115115.png]]
Вот новый сервис:
externalName - он хранит в себе адрес, т.е. это тот человек, кто делал презентацию настроил по такому названию - адрес.
![[Pasted image 20241212115407.png]]
И теперь для подключения мы указываем название нашего сервиса.
![[Pasted image 20241212115507.png]]

Чего мы этим добились? Мы вынесли адрес БД в сервис.

#### LoadBalancer
Балансирует нагрузку между нодами.
![[Pasted image 20241212120419.png]]

### Ingress Controller
Пришел тимлид, сказал, что выигрыша по скорости от того, что мы создали отдельный сервис и отдельную БД для хранения файлов не будет. Поскольку мы ему возвращали не id заметки, а саму заметку. А для этого наш сервис шел в сервис attachments и ждал, пока он вытащит из бд файл и ответит. Теперь же мы будем на сервисе notes возвращать id заметки пользователю, а он уже сам пойдет в сервис attachments, чтобы забрать файл по этому id. Тем самым, сервис notes не нагружается, поскольку он быстро вернул id заметки и не участвует в процессе передачи файла.
![[Pasted image 20241212121040.png]]
![[Pasted image 20241212121025.png]]

Имеем проблему. Вспомним, как мы получили доступ извне к notes? Мы создали NodePort сервис, у которого указали порт 30000. Клиент теперь должен обратиться по такому порту, чтобы получить доступ к notes. Но теперь мы хотим, чтобы клиент еще и мог обращаться к attachments извне. Нам опять создавать NodePort и указывать уже другой порт, т.к. теперь мы должны перенаправлять запрос клиента в attachments, а не в notes. А клиенту теперь нужно думать о том, чтобы если клиент делает запрос в notes, то нужен порт 30000, а если клиент делает запрос в attachments, то порт нужно указывать уже другой. И это не круто. Мы бы хотели, чтобы была одна точка входа, куда клиент мог обращаться и это точка входа, посмотрев на url, по которому клиент обращается решала, куда перенаправить запрос, в notes или в attachments, тем более у наших запросов есть подсказка. Запросы, у которых идет /notes принимает notes сервис, а запросы, у которых /attachments принимает attachment сервис.
![[Pasted image 20241212121221.png]]

И такая штука есть - Ingress контроллер.
![[Pasted image 20241212121553.png]]
![[Pasted image 20241212121613.png]]
В Ingress можно настроить все, что связано с безопасностью.

Создаем блок Ingress:
![[Pasted image 20241212121741.png]]
в spec:
host - хост, где наш сервис крутится. Если мы купили себе my-app.com, то здесь указываем my-app.com.
А дальше указываются правила. Если путь начинается на prefix /notes, то отправь такой запрос в сервис notes-svc
![[Pasted image 20241212121909.png]]

![[Pasted image 20241212121955.png]]

А для attachments в другой сервис:
![[Pasted image 20241212122013.png]]

Используется именно сервис ClusterIp, поскольку общение Ignis и пода происходит уже внутри kubernetes сети и поэтому нам не нужен NodePort.

Ingress по сути похож на Gateway из Spring Cloud. Но мы можем иметь и ingress и gateway. Например, все запросы из Ingnit будут идти в Gateway, который может как-то доработать запрос. В примере автора презентации у них был и Ingress и Gateway и в Gateway автор дополнял хедеры запросы информацией о пользователе и потом уже запросы направлялись в поды. Gateway ссылается на сервисы по их названиям.

