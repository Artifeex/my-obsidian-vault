Наши микросервисы находятся внутри одной сети. Эта сеть защищена от прямого попадания в нее запросов. Т.е. клиент напрямую не обращается в какой-то из наших микросервисов. В нашу сеть есть единая точка входа - Gateway, в которую клиент и обращается. Т.к. все запросы попадают сначала в эту точку входа, то мы можем в этом месте провести Security, Logging запросов и т.д. И если все успешно, то потом перенаправить запрос уже в какой-то конкретный микросервис. Трафик, который пришел извне от клиентов называется внешним трафиком.

Но помимо внешнего трафика есть еще и внутренний, т.к. микросервисы часто требуют общения друг с другом. Такой трафик называется внутренним. О внешнем трафике разговор будет в другой теме. Здесь же мы поговорим о внутреннем трафике.
![[Pasted image 20241204112853.png]]

### В данной секции рассмотрим следующие проблемы, которые у нас появляются. В скобочках указаны названия решения проблемы
![[Pasted image 20241204113704.png]]
1. Как микросервисы в одной сети узнают друг о друге, чтобы пообщаться? (Service discovery)
2. Как микросервисы, которые работают внутри сети узнают о новых инстансах микросервисов, которы мы решили создать из-за повышенной нагрузки? (Service Registration)
3. Как распределить нагрузку между несколькими инстансами одного и того же микросервиса? (Load balancing)
Если использовать какой-то стандартный подход из монолита - создать какой-то отдельный LoadBalancer, который будет иметь таблицу IP адресов. Но эту таблицу должен постоянно кто-то поддерживать, т.к. в микросервисах постоянно создаются и удаляются микросервисы. 
![[Pasted image 20241204115443.png]]
Как решить эту проблему?
Использовать Discovery Service и Service Registry. Discovery Service внутри себя использует Service Registry, в котором сохраняется информация о микросервисах и их IP адресах. 
При запуске микросервисы будут регистрироваться в Service Registry, передавая свой IP и порт. Чтобы поддерживалось актуальное состояние системы, микросервисы посылают на Service Registry heart beats, которые подтверждают, что микросервис все еще живой. Если микросервис долгое время не посылает сообщения о том, что жив, Service Registry считает, что он умер и удаляет его. 
![[Pasted image 20241204131420.png]]

### Client-Side Service Discovery
Микросервисы при старте регистрируются в Service Registry. Когда микросервис хочет пообщаться с другим микросервисом, то он идет в Service Registry и получает от него список IP адресов инстансов этого микросервиса. И дальше уже нашnd-Side, микросервис решает в какой инстанс сходить. Поэтому это и называется Client поскольку именно на клиенте решается в какой инстанс микросервиса пойти, т.е. балансировка происходит внутри микросервиса, который хочет пообщаться.
![[Pasted image 20241204133257.png]]
Главным плюсом Client-Side Service Discovery является то, что мы можем на клиенте самостоятельно выбрать алгоритм балансировки и даже написать свой собственный.
Минусом является, что нужно самим написать код для балансировки запросов(Хотя это довольно просто, если мы используем Spring Cloud).

Server-Side - это не наш отдельный сервис, который мы сами пишем, а, например, kubernetes, который будет заниматься балансировкой, поддерживать регистрацию и все остальное. Т.к. мы сами не пишем такой микросервис, а используем доп тулу - kubernetes, которая также где-то отдельно разворачивается, то поэтому это и называется ServerSide. 

![[Pasted image 20241204135047.png]]
![[Pasted image 20241204135008.png]]

Из интересного на стороне клиента есть кэш. Т.е. когда он захотел пообщаться с другим микросервисом, то в первый раз он идет сразу в Service Discovery Layer и получает список IP адресов и сохраняет в кэш. Благодаря этому, чтобы выполнить следующий запрос ему не нужно сначала идти в Service Discovery, чтобы получить IP, а можно использовать значения из кэша, что хорошо, т.к. нет промежуточного вызова, а также Service Discovery разгружается и не находится под постоянной нагрузкой. При этом кэш обновляется раз в заданное время(15-20 сек). Если же по какому-то IP адресу не получилось достучаться, то микросервис берет самый новый IP адрес, который у него сохранен в кэше и пытается обратиться по нему.

### Какие компоненты Spring нам понадобятся для реализации Client-Side Discovery Server подхода
![[Pasted image 20241204135945.png]]
Neflix(Open) Feign Client - аналог RestTemplate для отправки запросов. 

### Реализация
Создаем новый проект. В нем добавляем зависимости Eureka Server, Config Server(будем сохранять конфиг для этого проекта в Config Server), а также Spring Actuator(т.к. мы будем перед стартом наших микросервисов в docker compose проверять, стартанула ли eureka server).
Над классом вешаем аннотацию @EnableEurekaServer
![[Pasted image 20241204143320.png]]
В application.yml:
```yml
spring:  
  application:   
    name: "eurekaserver"  
  config:  # для коннекта с configserver
    import: "optional:configserver:http://localhost:8071/"  
  
management:  
  endpoints:  # publish spring actuator endpoints
    web:  
      exposure:  
        include: "*"  
  health:  
    readiness-state:  
      enabled: true  
    liveness-state:  
      enabled: true  
  endpoint:  
    health:  
      probes:  
        enabled: true
```
Файл конфигурации храним с использованием Server Configuration, который мы разбирали ранее на гитхабе:
![[Pasted image 20241204144947.png]]

### Настройка Eureka Client
Теперь у нас есть поднятный Eureka Server и нам нужно, чтобы наши микросервисы регистрировалсь на нем при старте.
Для этого нам нужна зависимость:
```xml
    <dependency>
      <groupId>org.springframework.cloud</groupId>
      <artifactId>spring-cloud-starter-netflix-eureka-client</artifactId>
    </dependency>
```
И в application.yml добавить:
```yml
spring:  
  cloud:  
    inetutils:  
      preferred-networks:  # это решало проблему с eureka dashboard
        - 192.168
management:  
  endpoints:  
    web:  
      exposure:  
        include: "*"  
  endpoint: # включили endpoint, который позволит HTTP запросом выключить наш микросервис  
    shutdown:  
      enabled: true  
  info: # в actuator включили info endpoint для получения информации о нашем сервисе. Eureka будет использовать его как раз для получения инфы  
    env:  
      enabled: true  
  
endpoints: # эта настройка нужна для старых версий SpringBoot. Если используем новую, то достаточно endpoint.shutdown.enabled=true  
  shutdown:  
    enabled: true  
  
eureka:  
  instance:  
    preferIpAddress: true # чтобы при регистрации микросервис передавал IP адрес, а не hostname  
  client:  
    fetch-registry: true # т.к. eureka client, то будем извлекать информацию о других микросервисах для общения с ними  
    register-with-eureka: true # сами должны также зарегистрироваться на eureka сервере  
    serviceUrl:  
      defaultZone: http://localhost:8070/eureka/ # адрес eureka сервера  
  
info: # Чтобы указать детали о нашем микросервисе в eureka dashboard. Ее получит eureka используя info endpoint в actuator  
  app:  
    name: "accounts"  
    description: "Accounts microservice description"  
    version: "1.0.0"
```
localhost:8070/eureka/apps - можно посмотреть расширенную информацию по микросервисам, которые зарегистрировались в Eureka.
В application.yml выше мы настроили actuator shutdown endpoint. Благодаря этому, если мы отправим POST запрос, то наш микросервис остановится:
![[Pasted image 20241204171127.png]]
Это более предпочитетельный вариант, т.к. в таком случае микросервис сначала проверит, что выполнил все задачи, а потом только завершит свое выполнение и сообщит Eureka, что он перестает работать.

### Наш микросервис каждые 30 секунд отправляет hearbeat о том, что он жив и работает в Eureka Server. Если мы остановим Eureka Server, то в микросервисах сможем увидеть, что выбросилось исключение, т.к. Eureka не отвечает. Видим, что отправляется PUT запрос
![[Pasted image 20241204172100.png]]
### Feign Client
```xml
    <dependency>
      <groupId>org.springframework.cloud</groupId>
      <artifactId>spring-cloud-starter-openfeign</artifactId>
    </dependency>
```
Когда мы используем RestTemlate, то нам нужно указывать IP, порт, параметры запроса. Мы же хотим автоматизировать этот процесс для общения с другими микросервисами. Для этого используется FeignClient, который похож на Repository интерфейс из Spring Data Jpa. В Feign Client мы пишем только описание методов, а реализацию на себя берет библитека.

Задача - хотим в accounts микросервисе пообщаться с cards микросервисом, вызвав GET метод на cards/api/fetch
Создаем интерфейс:
![[Pasted image 20241204174313.png]]
А вот метод контроллера. Видим, что его сигнатура совпадает с сигнатурой метода в FeignCliente(только тут не только сигнатура должна совпадать, но и возвращаемый тип).
![[Pasted image 20241204174345.png]]
А также мы должны навесить @EnableFeignClients в том микросервисе, который будет использовать FeignClient для общения с другими микросервисами.
![[Pasted image 20241204174625.png]]
FeignClient как транзитивную зависимость тащит Spring Load Balancer и FeignClient занимается балансировкой запросов под капотом.

Далее создали еще один контроллер, с помощью которого будем получать информацию от всех микросервисов(агрегировать) и возвращать пользователю.
![[Pasted image 20241204181413.png]]

И все, что нужно - это просто заинжекить FeignClient и вызвать тот метод, который мы написали в интерфейсе и все. 
![[Pasted image 20241204181509.png]]
### Объяснение той красной ошибки, которую мы можем увидеть в eureka dashboard
![[Pasted image 20241204182607.png]]
У Eureka есть так называемый self-preservation(само сохраняемый) режим. Eureka входит в такой режим, когда большинство микросервисов перестают отправлять hearbeats о том, что они живы. Но вместо того, чтобы удалять сразу все такие микросервисы Eureka переходит в self-preservation мод. И если клиент приходит для получения информации о каком-то микросервисе, чтобы с ним пообщаться, то Eureka выдает такую информацию, несмотря на то, что heartbeats от микросервисов могут не поступать. Зачем это нужно? Это нужно для того, чтобы смягчить проблему сети. Т.е. может быть ситуация, когда появились временные проблемы с сетью и микросервисы не могут отправить heartbean в Eureka, при этом это все еще здоровый микросервис. И когда проблемы с сетью закончатся, то снова все будет впорядке. И для того, чтобы Eureka в случае таких проблем с сетью не удаляла сразу всю информацию по микросервисам и нужен этот self-preservation режим. А красная надпись говорит о том, что произошел переход в этот режим и когда клиенты будут обращаться к нам для получения информации о микросервисах, с которыми они хотят пообщаться, то Eureka сообщает, что возможно, какая-то часть микросервисов уже не рабочая(т.к. hearbeats не доходят и  Eureka не может быть уверена в том, что микросервисы живы).

Т.е. Eureka не сразу удаляет микросервисы, которые не отправляют hearbeat, а дает им шанс отправить heartbeat за некоторый период. Например, за 90 сек микросервис должен отравить hearbeat, если не получилось, то тогда уже Eureka все-таки удалит такой микросервис.
![[Pasted image 20241204183146.png]]

Причем важно заметить, что в такой режим self-preservation она попадает только в том случае, если одновременно перестали посылать heartbeats сразу заданный процент микросервисов(по дефолту это 15%). Это как раз для того, чтобы определить сбой сети, т.к. если один микросервис перестал отвечать, то это, скорее всего, не сбой сети, т.к. это всего 1 микросервис, а если бы был сбой, то, скорее всего, сразу множество микросервисов перестало бы отвечать. И поэтому, если микросервис перестал посылать heartbeats, то мы его удаляем, не переходя в режим self-preservation.

Причем только после того, как Eureka перешла в self-preservation мод, только тогда перестают удаляться микросервисы из Eureka. Т.е. пока она не в self-preservation моде, то микросервисы продолжат удаляться.
Настройки, связанные с этим режимом:
![[Pasted image 20241204183927.png]]

### Как развернуть все в docker compose?
common-config.yml.
В common мы теперь для клиентов eureka, т.к. она запущена теперь не в localhost, а в контейнере, то должны указать в defaultZone не localhost, а название сервиса в docker-compose, поэтому мы и переписываем значение с помощью environment переменных.
![[Pasted image 20241204191421.png]]
Добавляем eurekaserver в docker-compose:
![[Pasted image 20241204191533.png]]
Указываем, что eurekaserver зависит от configserver, т.к. в confiserver хранится eurekaserver.yml, в котором находится:
![[Pasted image 20241204190315.png]]
И в микросервисы добавляем еще depens_on на eurekaserver, т.к. сначала должна запуститься она, а потом только микросервисы, которые в ней зарегаются. А также extends от microservice-eurekaserver-config, т.к. в нем находится путь для клиентом Eureka:
![[Pasted image 20241204191756.png]]
