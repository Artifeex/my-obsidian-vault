### 8. 001 Введение в проблему N + 1 запросов

Сначала мы пишем наше приложение как можно быстрее, а потом занимаемся оптимизацией SQL запросов для повышения производительности.

Получим юзера через session.get(User.class, 1L);

Выполнится 2 запроса, 1 на то, чтобы получить юзера и еще один, чтобы инициализировать OneToOne Profile profile поле у нашего User.

1-е правило гласит о том, что мы не должны использовать bidirectional связь(когда используем mappedBy) при OneToOne, если у сущности в OneToOne используется синтаксический(автогенерированный) первичный ключ.

![[image.png]]

2-е правило всегда и везде ставить LAZY инициализацию, где это возможно.

Невозможно сделать одновременно у нескольких коллекций в одной сущности фетч тайп EAGER, поскольку в одном запросе возвращается сразу множество строк и Hibernate не может понрять, какие строки относятся к UserChat, а какие к payments.

![[image 1.png]]

Что такое проблема N+1 запроса? Например, мы первым запросом вытащили N user. А у каждого юзера есть поле FetchType.EAGER, т.е. после запроса автоматически для каждого юзера делается еще доп запрос на получение значения для этого поля, помеченного EAGER. Тогда получается, что для N юзаеров сделается N запросов на получение транзитивного поля. И тогда получается суммарно N+1 запрос. N - доп запросов для EAGER поля и 1 запрос для получения пользователей. И если транзитивных полей будет несколько, то проблема из N+1 превращается в const * N + 1, т.к. каждое такое поле будут заставлять генерировать еще N запросов для каждого из пользователей.

### 8. 002 @BatchSize

Заменим у всех коллекций в User fetch.EAGER на fetch.LAZY. Получим список пользователей и у каждого получим getPayments(). Тогда выполнится N запросов, для каждого юзера свой запрос.

![[image 2.png]]

Попробуем оптимизировать. Давайте навесим аннотацию BatchSize над полем payments.

![[image 3.png]]

Получаем вот такой запрос. Как видим, payments одним запросом уже брались не для каждого отдельного пользователя, а они объединились в один запрос и с помощью in мы одним запросом получились payments сразу 3-х пользователей, а это как раз то значение, которое мы указали в BatchSize. Но наших пользователей было 5, поэтому сделался еще один такой же запрос, но уже всего 2 знака вопросика в in. В итоге вместо 5 запросов мы получили 2. Но проблема N+1 не решена, поскольку теперь это перешло в проблему 1 + N/batchSize.

![[image 4.png]]

К тому же, BatchSize не работает в случае Company, т.е. когда у нас ManyToOne поле. Но чтобы работало, нужно повесить аннотацию BatchSize не над полем, а над сущностью Company. Тогда это будет работать, причем везде, где будет ссылка на Company.

![[image 5.png]]

Таким образом, появляется правило: Если ManyToOne, то BatchSize ставим над классом сущности, а если OneToMany, то ставим аннотацию над полем. Но все равно, это хоть и улучшает ситуацию, но не решает проблему N+1.

### 8. 003 @Fetch

Навесим аннотацию Fetch над нашим полем с аннотацией OneToMany.

У Fetch есть несколько FetchMode. Для коллекций подходит только subselect.

![[image 6.png]]

![[image 7.png]]

- SELECT - делает по сути тоже самое, что и обычная логика. Просто создает дополнительный запрос для получения зависимостей.
- JOIN - вместо отдельного запроса мы делаем сразу JOIN с той сущностью, от которой зависим. Не работает с коллекциями, т.к. получается декартово произведение множество из-за нескольких JOIN.
- SUBSELECT - единственный FetchMode, который работает для коллекций.

В таком случае делает запрос 1 на получение пользователей, а потом происходит еще 1 запрос для получения payments и в where условии при сравнении id пользователя в in происходит подзапрос на тех пользователей, которых мы вернули в 1 запросе. Т.е. он просто копирует 1 запрос и вставляет его в подзапрос.

![[image 8.png]]

Но это опять не работает с company, которая является ManyToOne.

Но еще проблема в том, что такой механизм работает только если мы получаем сразу всех пользователей. Если нам нужно получить только 1 пользовател

Но что-то в конце он сказал, что тоже не стоит использовать Fetch как и BatchSize и есть более элегантное решение.

### 8. 004 Query Fetch

Используем fetch на уровне hql запроса.

![[image 9.png]]

Какой SQL получим?

Происходит INNER JOIN + при SELECT мы сразу извлекаем информацию для payments, поэтому они будут проинициализированы! Но мы не можем использовать LIMIT и OFFSET из-за того, что дублируются строки(я не понимаю, почему).

![[image 10.png]]

Если использовать просто JOIn без fetch, то тоже происходит JOIN, но в данном случае в select уже нет полей для получения payments. При этом такой запрос получается более эффективным, потому что он не возвращает дублирующие строки и поэтому можно использовать lIMIT и OFFSET.

![[image 11.png]]

Но мы так же можем сделать и с company, тогда она тоже будет использоваться в INNER JOIN и в SELECT! Но

![[image 12.png]]

### 8. 005 @FetchProfile

fetch работает только в HQL, criteria api, querydsl. Если мы просто будем получать какую-то сущность через session.get(), то он не будет работать. Но чтобы решить эту проблему, hibernate предлагает некоторые решения.

FetchProfile

![[image 13.png]]

Навешиваем аннотацию FetchProfile над классом User.

![[image 14.png]]

По поводу mode. Есть 3 мода

- Select - поле будет заполнено, но будет выполнен дополнительный запрос для получения данных для company.
- JOIN - сделает сразу джоин для нашей зависимости, т.е. в случае company при получении user, поле company так же будет заполнено(если выбрали данный профайл), использовав join.
- Subselect - только для коллекций, сделает еще подзапрос с джоином

![[image 15.png]]

Тогда у нас сразу же загрузится и компания одним запросом, используя JOIN. Что круто.

Но, если мы хотим еще и payments получить, а т.к. он у нас LAZY, то будет выполнен доп запрос. Тогда мы можем написать еще один профайл и в зависимости от наших нужд, активировать перед запросами один из профайлов и это круто! Т.е. когда нам нужно загрузить payments сразу же, то мы активируем соотвествующий профайл, а если не нужно, а только какая-то базовая информация нужна, то не активируем профайл и получается оптимизация!

Напишем такой профайл:

![[image 16.png]]

Тогда при получении user мы одним запросом получим и юзеров и company и payments, просто будут использованы JOIN.

Но это все работает только если мы работаме через get метод, если напишем HQL, то там автоматически не будут происходить JOIN, их нужно явно писать вместе с fetch.

### 8. 006 Entity Graphs
EntityGraphs решают проблему, что в HQL у нас есть fetch, то он не работает для hibernate методов, таких как get. А в Hibernate есть Profile, но он не работает в HQL. Мы же хотим использовать инструмент, который в одном месте настраивается и влияет и на HQL запросы и на hibernate методы.

Проблема прошлого способа в том, что при hql запросах не работает fetchprofile, он работает только при получении одной сущности через session.get(). Но есть EntityGraph, который помогает решить эту проблему.

![[image 17.png]]

Использование, метод get нам не подходит, потому что в него никак не передать пропертис, поэтому используем метод find.

![[image 18.png]]

Тогда автоматически загрузятся и компании и payments нашего пользователя.

И плюс этого способа в том, что он работает и при hql запросах, для этого нужно просто передать hint

![[image 19.png]]

Разберем разницу между load и fetch в GraphSemantic

![[image 20.png]]

LOAD - используем те фетч тайпы, которые расставили в наших сущностях.

FETCH - Все маппинги у нас LAZY, даже если мы где-то явно поставили EAGER

Мы так же можем указывать сабграфы, чтобы при загрузке транзитивных сущностей, мы могли сразу у транзитивных сущностей еще дополнительно инициализировать их транзитивные сущности, например, мы хотим при загрузке user загрузить его компании, а также usersChats сущности и при этом мы хотим, чтобы userChats, который хранит внутрь себя транзитивную сущность chat так же загрузилась!

![[image 21.png]]

Мы так же можем сделать тоже самое, но используя java код, а не аннотации:

![[image 22.png]]

Таким образом, это более гибкий подход, который позволяет загружать только то, что нам нужно и при этом этот механизм будет работать как для hql запросов, так и для стандартного использования hibernate с get методами.

### 8. 007 Best practices

![[image 23.png]]

1. Избегать bidirectional(mappedBY) связей, если наш ключ является синтетическим(генерируется на стороне БД). Если ключ не синтетический, то LAZY будет работать. В противном случае, всегда будет загружаться Profile, хоть ставь LAZY, хоть не ставь.

![[image 24.png]]

1. Использовать LAZY везде. EAGER срабатывает только при использовании hibernate.get() метода(в таком случае используется JOIN при получении нашей основной сущности), если же мы используем HQL, то EAGER уже срабатывать не будет. Точнее будет, но он будет работать не через JOIN, а мы столкнемся с проблемой N+1, когда у множества сущностей, которые мы получили будет дополнительно из-за EAGER фетч тайпа вызываться доп запросы для инициализации. Кратко: EAGER при get запросах сделает JOIN, но при этом мы не сможем использовать LIMIT, OFFSET, т.к. будет декартово произведение, а при HQL будет N+1 c доп запросами.
2. Не использовать BatchSize и Fetch. Поскольку это не работает на всех бизнес случаях, а решает какие-то конкретные + BatchSize не особо решает проблему, а только ее немного улучшает.
3. Используем ключевое слово fetch внутри HQL.
4. Лучше использовать EntityGraph, т.к. он будет работать и в HQL и в get методах
5. Как дополнение - сначала пишем бизнес логику, а потом уже начинаем оптимизировать все эти запросы.

### 9. 001 Введение в транзакции и блокировки

![[image 25.png]]

![[image 26.png]]

![[image 27.png]]

![[image 28.png]]

![[image 29.png]]

![[image 30.png]]

Если выполнился commit, то записи будут сохранены, даже если вырубился свет.

![[image 31.png]]

![[image 32.png]]

![[image 33.png]]

Транзакция А обновляет значение поля с 5 на 7 и делаем commit, а транзакция B так же обновляет это же поле с 5 на 10, но делает ROLLBACK. И тогда изменения откатываются и значение в поле становится равным 5. Т.е. потерялся результат работы транзакции А. В современных БД такое невозможно.

![[image 34.png]]

![[image 35.png]]

Транзакция B обновила поле с 5 на значение 7, а транзакция А прочитала эти данные и работает со значением 7. Но потом транзакция B отменяет свои изменения, а наша транзакция продолжает работать с неправильными данными. Т.е. для решения транзакция А должна видеть только те изменения, которые были до открытия транзакции А или данные, которая сама транзакция А изменила.

![[image 36.png]]

![[image 37.png]]

Транзакция А прочитала 5-ку, потом транзакция B сделала изменения этой 5-ки на 7-ку и сделала COMMIT. Потом транзакция А опять читает те же данные, а они не совпадают с тем, что она прочитала в первый раз.

Особый случай non repeatable read(это не совсем уже чтение, а просто вдвоем изменяем данные)

![[image 38.png]]

![[image 39.png]]

Коммит транзакции B происходит позже и перетирает изменения транзакции А.

![[image 40.png]]

![[image 41.png]]

Очень похожа на non repeadable read, но вместо обновления строк происходит добавление новых и хотя кажется, что проблемы примерно одинаковые - это далеко не так. Если в случае non repeadable read для решения проблемы нам достаточно просто заблокировать строки, которые мы читаем, чтобы туда не вносились изменения, тогда как для решения проблемы phantom read мы должны блокировать целые таблицы.

Для решения этих проблем были придуманы уровни изолированности транзакций

![[image 42.png]]

Потерянное обновление как мы видим не реализуется даже на самом низком уровне изолированности.

READ COMMITED - по умолчанию в hibernate.

Но чем выше уровень изолированности, тем хуже производительность, поскольку мы замедляем параллельное выполнение транзакций, вплодь до последовательного выполнения, как в случае SERIALIZABLE.

![[image 43.png]]

Особенности постгрессы в том, что read uncommited уровень изоляции не поддерживается, минимальный уровень изолированности это read commited, поэтому невозможно в постгрес получить проблему грязного чтения. Так же в постгресс не поддерживается repeatable read. Т.е. мы его хоть и можем поставить, но он будет работать почти как serializable. Т.е. по факту в постгресс есть 2 основных уровня изолированности read commited и serializable

### 9. 002 JPA Transactions

Есть JPA аннотация Transcaltional, которая говорит, что какой-то другой класс будет заниматься открытием транзакции(когда мы вызываем session.beginTranscation(), под капотом вызывается getTranscation + transcation.start() методы), комитом транзакции, а в случае каких-то проброшенных исключений rollback транзакций. Но, т.к. это только спецификация, кто-то еще должен предоставить ее реализацию, hibernate не предоставляет, но spring предоставляет.

![[image 44.png]]

Мы можем узнать текущий уровень изоляции, получив Connection из JDBC:

![[image 45.png]]

Когда все это было написано, еще не было enum, поэтому уровни изоляции представлены в виде int.

1 - read uncommited

2 - read commited(дефолтный уровень для postgres)

4 - repeatable read

8 - serializable

Так можно установить уровень изолированности транзакций через файлик с пропертями hibernate. Но лучше оставить по умолчанию и изменять уже в коде при запросах.

![[image 46.png]]

### 9. 003 Locks. Optimistic. Part 1

Когда мы делаем запрос, то можем передавать LockModeType

![[image 47.png]]

READ, WRITE - устаревшие, которым есть синонимы в виде OPTIMISTIC - READ, и OPTIMISTIC_FORCE_INCREMENT синоним для WRITE.

Таким образом, у нас есть 5 основных LockMode. 2 оптимистических(означает, что решаем проблемы транзакций через java приложение) и 3 пессимистических(решаем через СУБД).

OPTIMISTIC - по сути можем не указывать, т.к. просто говорим, что будем решать проблему на уровне java приложения(это по умолчанию такое).

Должны указать аннотацию над нашей сущностью, которые мы хотим получить через OPTIMISTIC LockMode

![[image 48.png]]

OptimisticType:

- None - нет никакой блокировки
- VERSION - в таблице появляется дополнительный столбец VERSION, в котором для каждой строчки будет записана версия и при измении строки эта версия будет увеличиваться. Т.е. в начале, например, версия 0, а потом, после изменения этой строки версия станет 1. Т.е. у нас явно в таблице должно быть такое поле и в классе, который соотвествует таблице.
    
    Тогда мы добавляем новое поле version и вешаем аннотацию Version.
    
    ![[image 49.png]]
    
    Чтобы hibernate сам автоматически создал таблицы по нашим Entity и в нашем случае добавил поле version:
    
    ![[image 50.png]]
    
    А после первого запуска значение create заменяем на validate, чтобы при каждом запуске он не создавал по новой нашу БД.
    
    ![[image 51.png]]
    
    И если какая-то другая транзакция на момент исполнения нашей транзакции уже обновила эту строчку, то значит, что версия изменилась, а если версия изменилась, то в условии where уже не будет такой строчки с такой версией, а это, значит, что update не обновит данные, при этом update возвращает количество обновленных строк и тогда мы можем проверить, что строк обновилось 0, а это значит, что кто-то другой уже обновил эту строку и можно выбросить OptimisticLockException. И этот эксепшн мы можем поймать и сообщить пользователю о том, что номер уже забронирован и выбери другой. Т.е. мы так решили проблему LAST COMMIT WINS(второй вариант проблемы unrepeatable read) и делаем FIRST COMMIT WINS.
    
    В БД мы увидим, что у всех payments версия 0, а у той, которую мы обновили значение 1.
    
    ![[image 52.png]]
    
    OPTIMISTIC_FORCE_INCREMENT - всегда инкрементируем версию, даже если не получилось обновить данные(optimistic же обновляем версию только тогда, когда реально удалось обновить данные).
    
    ![[image 53.png]]
    
    Вернем обратно LockModeType.OPTIMICTIC и попытаемся в двух транзакциях сделать изменения одной и той же сущности:
    
    ![[image 54.png]]
    
    Мы получим OptimisticLockException, который можем поймать в try catch. Вызвалось 2 update метода, первый обновил версию, и когда второй попытался обновить версию, то ничего не обновилось, т.к. в payment, который мы получили будет старая версия, а по ней и будем искать соотвествующую строчку, которой уже не будет из-за обновления первой транзакции.
    

### 9. 004 Locks. Optimistic. Part 2

На прошлом уроке мы узнали, что мы можем решить проблему LAST COMMIT WINS с помощью LockModeType.OPTIMISTIC, причем самое крутое в том, что нам для решения этой проблемы не потребовалось повышать уровень транзакции, мы работали на дефолтном уровне read commited, который не решает эту проблему. И это очень круто, т.к. повышается производительность приложения и решается такая проблема!

Теперь разберем, что делает OptimisticLockType.ALL

![[image 55.png]]

ALL в where условие при update кладет не версию(потому что это поле(соотвественно и столбец в БД) у нас пропадает), а использует значения всех столбцов. И в таком случае, если кто-то обновил данные для нашей строчки, то если мы попытаемся сделать обновление этой же строчки и в where будут находиться старые данные, то обновления так же не произойдет!

![[image 56.png]]

Мы добавили аннотацию DynamicUpdate, т.к. hibernate кэширует запросы на обновления и инсерт в БД и использует эти закэшированные запросы, а чтобы он не использовав кэшированные, а создал новые, где внутри where указал бы все наши поля при поиске строки для обновления, то нужно добавить такую аннотацию.

Теперь при update hibernate в where проверяет все поля, чтобы если кто-то изменил строку раньше нас, то мы не могли своим update перетереть те изменения, поскольку в данном случае уже у нас устаревшие данные.

![[image 57.png]]

При этом мы уже не указываем при find LockModeType. И если мы щас попытаемся выполнить такой код, то снова выбросится исключение.

![[image 58.png]]

OptimisticLockType.DIRTY отличается от OptimisticLockType.ALL тем, что в where кладет не все поля для сравнения, а только те, которые мы изменили в рамках нашей транзакции. Т.е. если мы изменили значения поля amount, то в where будет сравнение на id + amount, поля же receiver_id не будет, т.к. мы его не меняли.

![[image 59.png]]

На практике DIRTY не нужно использовать. ALL намного лучше, но вообще, лучше всего использовать OptimisticLockType.VERSION.

### 9. 005 Locks. Pessimistic

Pessimistic - решаем уже проблему на уровне СУБД, а не приложения.

![[image 60.png]]

![[image 61.png]]

В результаты мы попадем в deadlock. Вторая транзакция ждет, когда первая транзакция снимет свою блокировку, а первая транзакция ждет, когда завершится комит второй транзакции.

Но как происходит такая блокировка ? Посмотрим на код первой транзакции, которая совершила блокировку. В конце запроса используется for share

![[image 62.png]]

for share - это как раз блокировка нашей строки. И есть и другие виды блокировок в нашем случае использовалась именно for share, в документации можно посмотреть на другие. Особенность for share в том, что она блокирует UPDATE, DELETE, а SELECT не блокирует. В нашем случае был UPDATE, поэтому мы заблокировались и именно поэтому мы заблокировались на строчке с коммитом транзакции, а не на строчке с получением! Т.к. получение не блокирует в случае for share.

![[image 63.png]]

Далее PESSIMISTIC_WRITE. Снова подвисли, в SQL коде видим, что теперь добавилось не for share, а for update:

![[image 64.png]]

for update - блокирует так же UPDATE, DELETE и еще некоторые другие, в общем, он более строгий, чем for share.

PESSIMISTIC_FORCE_INCREMENT - в конце SQL запроса также добавит for update, но еще помимо этого испоьзуется версионность, как при использовании OPTIMISTIC_FORCE_INCREMENT. Т.е. мы опять должны добавить поле Version. Используется только если нам еще нужна дополнительная версионность, т.к. for update в любом случае обеспечивает нам нужный уровень изолированности.

Так же мы можем использовать локи при работе с HQL, тогда, если вернулось множество строк, то мы все эти строки заблокируем!

![[image 65.png]]

Но как теперь избавиться от зависний. Для этого можно у блокировки установить Timeout, после которого по истечению этого timeout, если не удалось завершить сессию, то выбросится исключение.

![[image 66.png]]

### 9. 006 Read Only Transactions

Если мы делаем SELECT запросы, а мы делаем их довольно часто и знаем, что никак не будет изменять данные, а мы чисто их только получили, то мы можем установить readOnlyMode, который отключает у hibernate проверку на то, изменился ли объект или нет, чтобы если изменился, то сессия стала dirty и при flush выполнить соотвествующие обновления.

Выключить можно с помощью

- session.setReadOnly(Object entityOrProxy, boolean readOnly) - для переданной сущности установить рид онли мод.
- session.setDefauiltReadOnly(true) - тогда все сущности, которые мы будем получать в рамках этой сессии будут рид онли
- Еще при hql так же вызываем setReadOnly, тогда все полученные сущности получат readOnly мод.

![[image 67.png]]

До этого мы сделали оптимизацию на уровне приложения. Т.е. мы просто hibernate сказали не проверять объекты на изменения.

Но мы можем открыть транзакцию на уровне БД с readOnly. И тогда, даже если мы пошлем запрос на обновление в рамках сессии, то выбросится исключение.

Для этого мы нативно делаем запрос в рамках транзакции. И если мы выключим readOnly мод, то при setAmount hibernate попытается сделать update и получим exception на уровне БД.

![[image 68.png]]

Зачем это нужно? Во-первых, ограничить от случайных изменений в своем приложении, во-вторых, некоторые СУБД(постгрес не входит в их число) делают какие-то оптимизации, когда устанавливается READ ONLY транзакции.

### 9. 007 Nontransactional Data Access

Мы можем не создавать явно транзакцию, т.е. не вызывать session.beginTransaction(). Тогда мы можем сделать 1 запрос и он выполнится, при этом все что мы получим так же как и обычно будет попадать в persistentcontext. Но лучше так не делать, либо максимум какие-то READ запросы.

### 10. 001 Entity Callbacks

callback - обратный вызов. В hibernate есть интерфейс callback

![[image 69.png]]

Колбеки нужно для того, чтобы перехватывать какие-то события из нашего жизненного цикла сущности и что-то с ним сделать.

Какие именно события мы можем перехватить определяет как раз CallbackType

![[image 70.png]]

PRE_UPDATE - перед тем как обновили

POST_UPDATE - после того как обновили

PRE_PERSIST - перед тем, как сохранили

POST_PERSIST - после того, как сохранили и т.д.

![[image 71.png]]

например, мы хотим реализовать функционал при котором перед сохранением Payment записи в БД мы сохраняли в объекте время создания, для этого пишем метод и помечаем его аннотацией PrePersist.

![[image 72.png]]

И при вызове find мы попадем в метод prePersist и видим, что все поля null, значит, мы попали в событие, которое предшествует сохранению.

![[image 73.png]]

Все такие колбеки хранятся внутри класса CallbackRegistryImpl и у него для нашего класса вызываются все сохраненные коллбеки. Ключом является класс, а значением массив колбеков нашего класса для определенного события.

![[image 74.png]]

Более логично перенести такой функционал в базовый класс. PreUpdate - вызовется, когда мы попытаемся обновить какое-то поле нашего объекта, когда он находится в persistent context.

![[image 75.png]]

Но на самом деле лучше выносить такие методы из классов Entity, т.к. это уже доп функционал какой-то, а в Entity должны быть поля(данные). Поэтому такие методы обычно выносят в классы листенеры, речь о которых пойдет на следующем уроке

### 10. 002 Listener callbacks

Создадим класс листенер и перенесем туда наши методы, единственное изменение в том, что теперь мы принимаем entity для которой вызвался коллбек

![[image 76.png]]

Чтобы такой листенер заработал нам нужно повестить аннотацию EntityListeners над классом Entity, над которым мы хотим, чтобы срабатывал наш листенер. В данном случае, мы хотим, чтобы он срабатсывал для всех, кто наследуется от AuditableEntity, поэтому повесили над этим базовым классом.

![[image 77.png]]

Таким образом, когда мы вызовем метод [session.save](http://session.save) вызовется этот метод pre_persists

Когда еще полезно? Например, мы хотим всегда знать количество участников в чате. Но, чтобы получить число участников, нужно будет получать много записей в UserChat и потом брать size. Но мы можем создать отдельное поле для сущности chat с количеством пользователей. И при добавлении новой записи в usersChats(т.е. при вызове POST_PERSIST)инкрементить значения количества пользователей.

![[image 78.png]]

![[image 79.png]]

![[image 80.png]]

Но важно работать с нашими usersChats, только через hibernate, а не через sql, иначе мы нарушим наш counter - и это та сложность, которую придется платить за такой крутой функционал.

Еще примером может быть, когда мы создали пользователя нового, то в листенере можем отправить ему сообщение на почту.

### `POST_PERSIST`

- **Когда срабатывает**: Событие `POST_PERSIST` срабатывает **после того**, как новая сущность была сохранена в базе данных, то есть после выполнения операции `persist`. Это означает, что сущность уже была записана в базу данных, и теперь можно выполнить какой-то код в ответ на это действие.
- **Использование**: `POST_PERSIST` используется для выполнения каких-либо действий после того, как сущность была успешно сохранена. Например, можно отправить уведомление или выполнить дополнительную логику.

### `POST_LOAD`

- **Когда срабатывает**: Событие `POST_LOAD` срабатывает **после загрузки** сущности из базы данных. Это происходит, когда Hibernate или JPA провайдер завершает загрузку сущности и полностью инициализирует ее, но до того, как сущность будет возвращена или использована приложением.
- **Использование**: `POST_LOAD` используется для выполнения действий после того, как сущность была загружена из базы данных. Например, можно восстановить состояние, выполнить какие-то вычисления на основе загруженных данных, подготовить данные для отображения и т.д.

`PRE_PERSIST` — это событие жизненного цикла сущности в JPA, которое срабатывает **до** того, как новая сущность будет сохранена в базе данных (до выполнения операции `persist`). Это позволяет выполнять определенные действия или модификации сущности перед её сохранением.

### 10. 003 Event Listeners

Есть более лучшее средство с колбеками, которое принято в Hibernate и не только.

В hibernate на самом деле очень много листенеров, например, даже при вызова метода save, мы не просто вызываем этот метод, а генерирует колбек, которые уже должен словиться листнерером и выполнится. И таких листенеров в hibernate много и самое прикольное, что мы можем сами создать класс, реализовать нужный интерфейс и добавится к множеству листенеров hibernate. Сами листенеров в hibernate делятся на группы, схожие с теми, что мы изучали ранее(POST_UPDATE и т.д.)

Вот так примерно выглядит хранилищие листнереров:

![[image 81.png]]

Сами листенеры - это функиональные интерфейсы, в которых нужно переопределить один метод, который и занимается прослушиванием события и выполняет некоторый функционал.

![[image 82.png]]

Т.е. мы можем добавлять листнеры(subscribe), либо удалять (unsubscribe). event - это всего лишь dto, в котором просто какая-то информация о событии.

![[image 83.png]]

Дальше он прикольно показывает, как можно создать Listener, который будет добавлять в новую таблицу Audit информацию о том, что были выполнены некоторые изменения с некоторой сущностью!

### 11. 001 Hibernate Envers. Part 1

До этого в уроке 003. EventListeners мы написали свой листенер, который нужен для аудита нашей системы. Но есть уже готовые аудиты, нужно только подключить зависимости.

![[image 84.png]]

Далее нужно навесить аннотацию Audited и с этого момента Payment аудируется.

![[image 85.png]]

В общем, Envers это про автоматические аудирование наших таблиц. Если понадобится - посмотришь видео.

### 12. 001 Second Level Cache. Конфигурация

![[image 86.png]]

PersistenceContext - это Map, в котором ключом является класс + id, а значением сущность, которую достали из БД.

FirstLevelCach включен всегда. А SecondLevelCache нужно уже подключать.

Сам кэш разбит на регионы и они нужны для хранения сущностей(аналог persistentContext). Т.е. это такой же map, где ключом является id, а значением не сама сущность, а ее сериализация. Т.е. при работе с лвл2 кэш будет постоянно происходить сериалезация и десериализация.

![[image 87.png]]

Зачем разбивать на регионы?

Мы можем разбивать на регионы по количеству занимаемой памяти, в каждом регионе можно настроить сколько времени будет жить сущность. Обычно для каждой сущности добавляется свой регион и если у нас будет 10 сущностей, то будет 10 регионов.

Cache Miss - не нашли нужную сущность в кэш.

Cache Put - положить сущность в кэш, после того, как случился Cache miss.

Cache Hit - когда нашли нужное в кэше, а не обратились в БД.

Как подключить кэш? Rласс RegionFactoryTemplate занимается созданием регионов.

![[image 88.png]]

А так же есть интерфейс StorageAccess, который как раз занимается тем, что умеет забирать данные из кэша, чистить их, добавлять в кэш.

![[image 89.png]]

Но до этого были абстрактный класс и интерфейс, который предоставляет нам hibernate. Но нам нужна какая-то реализация, давайте используем библиотеку jcache:

![[image 90.png]]

Т.е. jcache реализует как раз интерфейс и абстрактный класс, а под капотом использует ehcache.

В проперти файла hibernate мы должны указать, что хотим использовать лвл 2 кэш и указать провайдера кэша:

JcaheRegionFactory - это как раз класс, который нам достался из зависимости jcache и он является наследником абстрактного класса RegionFactoryTemplates.

![[image 91.png]]

### 12. 002 Second Level Cache. @Cache

Чтобы кэш второго уровня заработал нам нужно повесить аннотацию над сущностью, у которой мы хотим, чтобы работал кэш. И так же обязательно нужно указать usage, а какие виды бывают чуть позже разберем.

![[image 92.png]]

![[image 93.png]]

При этом user2(взяли из лвл2 кэш) и user будут разными объектами в java, хоть и с одинаковым содержимым! Почему? Потому что в кэше 2 уровня лежит десериализованная форма нашего объекта и когда происходит cache hit, то создается новый объект из этой десереализованной формы.

![[image 94.png]]

Если мы попытаемся получить company у закэшированной сущности, то все равно будет вызван запрос в БД, т.к. company не кэшируется! Мы при сохранении в лвл2 кэш сохранили только id company, но в лвл2 кэше нигде не сохранена по такому id компания, поэтому приходится идти в БД. Но мы можем закэшировать компанию и тогда запроса в БД не будет, т.к. при getCompany() мы возьмем закэшированный id компании и сначала по такому id попробуем сходить в кэш 2 уровня и если в каком-то регионе окажется компания с таким id, то она вернется и никакого запроса в БД!

![[image 95.png]]

Поэтому навешиваем Cache аннотацию над company:

![[image 96.png]]

Но еще важно, что если бы мы в первой сессии не сделали запрос на company, тогда бы она не положилась в lvl2 кэш и тогда во второй сессии при получении company запрос в БД бы совершился. Мы кладем сущность в кэш только тогда, когда делаем запрос!

А что, если мы хотим закэшировать еще и userChats. Тогда нам нужно навесить сразу 2 аннотации. Первая аннотация над список userChats, чтобы при кэшировании сущности User закэшировались все id userChats в отдельно созданном регионе! А вторую аннотацию нужно поставить над UserChat, чтобы сами UserChat были так же закэшированы и тогда можно было бы пройтись по закэшированным id userChats и потом по ним найти отдельным закэшированные сущности UserChat.

![[image 97.png]]

Вот, например, что будет если мы повесим аннотацию Cache только над полем userChats, а над классом UserChat не поставим. В кэше будут сохранены только id, потом по ним мы попытаемся найти сущности UserChat, но в кэше их не будет, т.к. мы не поставили аннотацию, чтобы они кэшировались и поэтому буду обращения в БД.

![[image 98.png]]

А вот, что будет, если не поставили аннотацию над List< UserChat> userChats, но поставили над классом UserChat. В таком случае, будут кэшироваться сущности UserChat, но сами id, который связаны с User не будут кэшироваться и поэтому будет запрос в БД для получения id, а уже по этим id не будет запросов в БД, поскольку мы сохранили сущности в лвл2 кэше.

![[image 99.png]]

### 12. 003 Second Level Cache. Regions

Остановимся более подробно над тем, что такое Regions.

Region - это просто область памяти в нашем кэше, в котором хранятся сереализованные сущности.

И Region еще определяют, насколько каждый из них большой, сколько по времени будут в нем хранится объектики.

Если мы сохраняем User и не указываем регион(а мы до этого не указывали), то создается регион по умолчанию и туда кладется сериализованный user.

Т.к. кэш 2 уровня относится к нашей сессии, то мы можем у нее его и получить:

![[image 100.png]]

И вот как раз можно посмотреть регионы, которые создались автоматически, если мы явно не указали какой-то регион для наших сущностей, которые мы закешировали, добавив аннотацию Cache.

![[image 101.png]]

Чтобы указать название регионы мы добавляем в аннотации Cache:

![[image 102.png]]

И если мы еще у какой-то сущности над которой повесили аннотацию Cache укажем region = “Company”, то эта сущности при кэшировании так же попадет в регион с именем Company!!!

Но, лучше так не делать. Т.к. мы можем настраивать регионы по отдельности и лучше для каждого отдельного типа сущности использовать свой регион.

Но как настроить регион?

В случае ehcache мы должны настраивать регион с помощь xml маппинга(то, как настраивать регионы зависит от провайдера кэша). На картинке ниже ошибка, мы создали не 2 кэша и их настройки, а создали 2 региона и задали их настройки.

![[image 103.png]]

Но, чтобы ehcache знал про такой файлик и взял из него конфигурацию, мы должны ему об этом сказать, использую hibernate.cfg.xml и добавив следующее поле

![[image 104.png]]

И чтобы при кэшировании сущности использовали именно те регионы, которые мы настроили, мы должны в аннотации указать region с тем названием, которое в файлике с настройками регионов.

![[image 105.png]]

И вот в списке регионов появились те регионы, которые мы создали в конфигурационном файлике.

![[image 106.png]]

CacheConcurrencyStrategy - то, как работают транзакции с нашим кэшем(это ведь одно пространство), а транзакций много. Т.е. по сути реализуют наши локи и одновременную работу с одной и той же сущностью в нашем кэше

CacheConcurrencyStrategy:

- ReadOnly - сущность полностью рид онли и мы не можем ее изменить. Используем такое, если сущность действительно является рид онли, например, это какой-то справочник, в который не будут вноситься какие-либо изменения. И плюсом является то, что не используется никаких блокировок, т.е. очень быстро работает. (но похоже, мы можем удалить и добавлять такие сущности, но главное, что не изменять)
- READ_WRITE - можем удалять, добавлять, изменять. Но тут уже access(strict) - это говорит о том, что если какая-то транзакция взяла лок на такой сущности, чтобы ее как-то изменить, то все другие транзакции, при обращении к данной сущности попадают в лок и вместо того, чтобы ждать его освобождения делают запрос в БД. И после того, как та транзакция, которая работала с сущностью сделает commit, то только тогда в кэш кладутся обновленные данные по нашей сущности
- NONSTRICT_READ_WRITE - рекомендует использовать ее. Она более оптимизирована, чем READ_WRITE, особенность в том, что в кэше даже когда с сущностью работает транзакция, другая транзакция все равно так же может взять какие-то данные, да, они могут оказаться старыми, но из-за этого огромный буст по производительности
- Transactional - используется для распределенных транзакций. Это значит, что есть 2 различные БД, которые находятся на разных машинах и вам нужно провести транзакцию в рамках 2-х машин.

![[image 107.png]]

### 12. 004 Second Level Cache. Query Cache

SecondLevelCache позволяет кэшировать наши SQL запросы, а не только сущности. Т.е. кэшируется результат каких-то запросов. Но важно то, что кэшируются только id! Т.е. если мы хотим закэшировать сущности, которые соотвествуют этим id, то нужно над этими сущностями так же использовать аннотацию Cache. Причем кэшируется именно готовый запрос, т.е. когда все параметры в нем расставлены, т.е. вместо знаков вопросика(если вспомнить про preparedStatement) мы уже использует какие-то конкретные значения.

![[image 108.png]]

Указываем, что будем использовать кэш для запросов

![[image 109.png]]

Чтобы запрос закэшировался мы должны указать setCacheable

![[image 110.png]]

И свойство setCacheable мы должны указывать каждый раз, если хотим, чтобы наш запрос либо закэшировался, либо, если он уже закэширован нашелся в кэше. Если не указать, то всегда будет запрос к БД.

Так же можно получать статистику по кэшам(но это сильно замедляет, поэтому на проде нельзя такое исползовать)(нужно еще добавить проперти для включения статистики).

![[image 111.png]]

### 13. 001 DAO & Repository. CRUD

Hibernate будет использовать на уровне dao и на service.

![[image 112.png]]

Создадим отдельный интерфейс Dao:

![[image 113.png]]

Создадим класс Dao для работы с Payment:

![[image 114.png]]

![[image 115.png]]

![[image 116.png]]

Но, чтобы не дублириовать код логично вынести этот функционал в базовый класс:

![[image 117.png]]

![[image 118.png]]

![[image 119.png]]

И теперь мы можем очень легко реализовывать множество dao для сущностей, которым нужны CRUD операции

![[image 120.png]]

![[image 121.png]]

![[image 122.png]]

На самом деле, получились не совсем dao объекты. dao обычно занимались маппингом из БД сущности в entity и наоборот. И то, что мы написали - правильнее назвать repository. Т.е. это дополнительный слой, который стоит между Service и Dao. Этот слой работает с dao. Поэтому переименовываем

![[image 123.png]]

![[image 124.png]]

И теперь воспользуемся нашим репозиторием

![[image 125.png]]

Но если мы так все оставим, то получим ошибку LazyInitializationException. Почему так? Потому что мы в наших методах закрывали сессию, хотя на самом деле, сессия еще нужна будет и дальше, например, для получению маппингов, если была инициализация Lazy. Поэтому на следующим занятии мы посмотрим, как нужно правильно сделать и работать с сессией.

### 13. 002 DAO & Repository. CurrentSessionContext

Теперь в наших методах репозитория мы не будем закрывать сессиию, но и не будем открывать новую, использует createSession. Вместо этого мы будем получать текущую сессию. Но, где хранится эта текущая сессия и кто за нее отвечает ?

![[image 126.png]]

Существует 3 основных контекста, которые занимаются хранением сессии

![[image 127.png]]

JTASessionContext - это для распределенных транзакций, нам не поможет.

ThreadLocalSessionContext и ManagerSessionContext очень похожи. Они оба хранят нашу сессию в ThreadLocal переменной. Т.е. это переменная, которая является локальной для нашего потока. Нам больше подходит ThreadLocalSessionContext. Он при beginSession(), создает ее, если она еще не было создана, а при комите удаляет сессию.

Вот тред локал переменная, которая лежит в нашем ThreadLocalSessionContext:

![[image 128.png]]

ManagedSessionContext - так же хранит такую перменную, но его минус является то, что он автоматически не создает сессию и не закрывает ее при вызове beginTransaction и commit, rollback.

Чтобы использовть такой контекст, мы должны явно это указать в классе с конфигурациями hibernate:

![[image 129.png]]

И теперь мы работаем с помощью этого контекста и самое прикольное, что даже в main нам теперь не нужно открывать и закрывать сессию. Мы получаем сессию с помощью getCurrentSession() и при beginTransaction, если сессия еще не было создана в нашем контексте, то она создается, а при commit, она закрывается так же с помощью ThreadLocalContext.

![[image 130.png]]

И поэтому вся работа с транзакциями происходит на уровне сервиса. На этом уровне мы открываем транзакцию, дальше, используя репозиторий выполняем все, что хотим с нашей БД и потом закрываем транзакцию так же на уровне сервиса, и тогда все методы репозитория будут работать с одной транзакцией.

Еще, что мы можем улучшить - так это перестать передавать внутрь Repository целый sessionFactory. Нам достаточно туда передать Session, внутри которой мы работаем. Но если мы хотим прям еще сильнее абстрагироваться от реализации, то можем испоьзовать EntityManager - интерфейс из JPA, который реализует класс Session в hibernate. Автор решил использовать именно этот интерфейс, а т.к. он испоьзует его, то вместо hibernate методов он вызывает jpa методы(например, вместо save вызываем persist)

![[image 131.png]]

![[image 132.png]]

Теперь внутри репозиториев мы передаем EntityManager

![[image 133.png]]

![[image 134.png]]

Но мы не можем теперь передать просто session внутрь Repository, потому что если мы работаем в многопоточной среде и другой поток обратиться к нашему репозиторию и что-то у него вызовет, то произойдет ошибка, т.к. тредлокал переменная session не будет создана у другого потока. Мы ее создали в первом потоке, потом уничтожили и сессия закрылась. Поэтому так нельзя, вместо этого мы должны передать туда прокси объект, который будет вызывать метод getCurrentSession.

![[image 135.png]]

![[image 136.png]]

### 13. 003 DAO & Repository. Практика

Как работать с DAO и Repository на уровне сервисов.

Создаем UserRepository

![[image 137.png]]

Создадим сервис, который будет работать с репозиторием. Причем в одном сервисе может быть множество репозиториев.

Для метода find нашего сервиса мы должны создать новый класс UserReadDto, поскольку с сервисом работают вышележащие уровни и они из соображений безопаности(не дожлны получить какие-то данные) или просто, чтобы меньше данных передавать между слоями, или из-за lazy initialization можем что-то запросить на уровне сервисов(или выше), где может быть закрыта сессия.

![[image 138.png]]

![[image 139.png]]

![[image 140.png]]

Чтобы работать на уровне интерфейсов создадим интерфейс с этим методом. И мы можем улучшать данный интерфейс, добавляя в него новые методы.

![[image 141.png]]

![[image 142.png]]

![[image 143.png]]

![[image 144.png]]

Есть фрейворки для того, чтобы не писать вручную мапперы - MapStruct.

![[image 145.png]]

С помощью графов мы можем улучшить перформанс нашего приложения и решать, нужно ли доставать lazy штучки или нет.

Для этого немного меняем интерфейс, теперь у нас будет два метода findById и основным будет тот, который принимает properties.

![[image 146.png]]

Дальше менярем реализация репозиторию UserRepositoryL

![[image 147.png]]

Создаем граф над сущностью user, чтобы загружать юзера и его компанию одним запросом:

![[image 148.png]]

И используем этот граф при запросе в UserService:

![[image 149.png]]

Мы можем еще сильнее улучшить гибкость нашего приложения, написав метод, который может вернуть любой тип данных из нашего UserService, главное передай маппер, если вдруг тебе понадобится какой-то другой dto, то всего лишь нужно будет написать маппер и передать его + создать сам класс dto. И это круто!

![[image 150.png]]

Перейдем к созданию метода create. Нам сверху придет dto, из которого мы должны получить всю необходимую информацию для сохранения в БД

![[image 151.png]]

![[image 152.png]]

![[image 153.png]]

![[image 154.png]]

![[image 155.png]]

Лучше создавать отдельные dto для кажой операции с нужными полями для этой операции.

Осталась последняя проблема - это то, что мы в ручную открываем и закрываем транзакции. Для решения этого используется аннотацию Transactional, которая используется на уровне сервисов. Но такого функционала нет, т.е. то, что мы навесили такую транзакцию ничего не дает, нужна библиотека, которая обеспечивает этот фукниоцанл. Например, в спринг такой функционал есть. В нашем же случае напишет такой функционал самостоятельно

![[image 156.png]]

### 13. 004 DAO & Repository. JSR 303. Bean Validation

Суть JSR303 - это то, что мы будет использовать готовые аннотации и объекты для валидации.

![[image 157.png]]

Используем аннотации из подключенной библиотеки

![[image 158.png]]

Валидацию лучше использовать на уровне сервисов. Поэтому лучше все эти аннотации использовать над DTO объектами.

![[image 159.png]]

Т.к. теперь работаем на уровне сервисов, то нужно создаавать свой валидатор

![[image 160.png]]

Мы пробрасываем исключение выше, чтобы уже на более высоком уровне отобразить пользователю ошибку.

Все поля, у которых не указана группа в аннотации - всегда валидируются. Если же группа указана, то данное поле валидируется только в том случае, если при валидации была передана данная група.

![[image 161.png]]

Передача группы при валидации:

![[image 162.png]]

А UpdateCheck - просто интерфейс метка:

![[image 163.png]]

### Некоторые обобщения

### Типы связей и поведение загрузки в Hibernate

1. `**@OneToOne**`
    - **По умолчанию**: `FetchType.EAGER`
        - Hibernate по умолчанию загружает связанные сущности немедленно. То есть, если у вас есть связь `@OneToOne`, связанная сущность будет загружена сразу же при загрузке основной сущности.
    - **Можно изменить на**: `FetchType.LAZY`
        - Если вы укажете `FetchType.LAZY`, связанная сущность будет загружена только при первом доступе к ней. Однако для `@OneToOne` связи это требует применения дополнительных техник, таких как прокси или `bytecode enhancement`, поскольку ленивую загрузку сложно обеспечить на уровне архитектуры без таких подходов.
2. `**@OneToMany**`
    - **По умолчанию**: `FetchType.LAZY`
        - Связанные сущности не загружаются сразу, а только при первом обращении к коллекции. Это делает работу с большими наборами данных более эффективной, так как не загружается весь набор связанных сущностей при каждой выборке.
    - **Можно изменить на**: `FetchType.EAGER`
        - При использовании `FetchType.EAGER` Hibernate загружает все связанные сущности немедленно при загрузке основной сущности, что может привести к декартову произведению в результате выполнения `JOIN`, как обсуждалось ранее.
3. `**@ManyToOne**`
    - **По умолчанию**: `FetchType.EAGER`
        - Hibernate немедленно загружает связанную сущность. Например, если сущность `Order` имеет связь `@ManyToOne` с `Customer`, то `Customer` будет загружен вместе с `Order`.
    - **Можно изменить на**: `FetchType.LAZY`
        - При указании `FetchType.LAZY` связанная сущность будет загружена только при первом обращении к ней. Это позволяет избежать излишней загрузки данных, которые могут не понадобиться.
4. `**@ManyToMany**`
    - **По умолчанию**: `FetchType.LAZY`
        - Связанные сущности не загружаются сразу, а только при первом доступе к коллекции. Это поведение помогает избежать загрузки большого количества связанных сущностей, что особенно полезно при сложных и больших взаимосвязанных структурах данных.
    - **Можно изменить на**: `FetchType.EAGER`
        - При указании `FetchType.EAGER` все связанные сущности будут загружены немедленно при загрузке основной сущности, что может вызвать значительные накладные расходы на производительность, особенно при работе с большими наборами данных.

### Книги

![[image 164.png]]