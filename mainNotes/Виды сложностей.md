### Амортизированная сложность — это средняя сложность выполнения операции в последовательности операций, рассчитанная с учетом их взаимодействия.

Амортизированная сложность отличается от худшей и средней сложности тем, что учитывает **общую стоимость выполнения нескольких операций, а не отдельной операции в худшем или среднем случае**.

---

### Типы временной сложности

1. **Худшая сложность (Worst-case complexity)**
    
    - Определяет максимальное время выполнения операции в самых неблагоприятных условиях.
    - **Пример**: Поиск элемента в несортированном массиве имеет худшую сложность O(n), если искомый элемент находится в конце массива или отсутствует.
2. **Средняя сложность (Average-case complexity)**
    
    - Определяет среднее время выполнения операции, учитывая равновероятные варианты входных данных.
    - **Пример**: Вставка элемента в хэш-таблицу в среднем занимает O(1), если хэши распределены равномерно.
3. **Лучший случай (Best-case complexity)**
    
    - Определяет минимальное время выполнения операции в самых благоприятных условиях.
    - **Пример**: Поиск элемента в отсортированном массиве (бинарным поиском) может завершиться за O(1), если искомый элемент находится в середине массива.
4. **Амортизированная сложность (Amortized complexity)**
    
    - Рассматривает последовательность операций, распределяя затраты на дорогостоящие операции по всей последовательности.
    - **Пример**: Вставка в конец динамического массива (`ArrayList`) амортизированно O(1), хотя увеличение емкости иногда требует O(n).

---

### Когда используется амортизированная сложность?

Амортизированная сложность важна для структур данных, где операции могут иметь различную сложность в зависимости от состояния структуры, например:

1. **Динамические массивы (`ArrayList`)**
    
    - Добавление в конец амортизированно O(1), так как копирование элементов при увеличении емкости происходит редко.
2. **Стек с увеличением емкости**
    
    - Если стек реализован на массиве, добавление нового элемента обычно занимает O(1), но увеличение емкости может занять O(n).
3. **Суксессивное удаление из связного списка**
    
    - Если список поддерживает сохранение указателя на последний элемент, удаление последовательности элементов будет O(1) для каждой операции.
4. **Сложные структуры данных, такие как**:
    
    - **Фибоначчиева куча** (amortized O(1) для добавления и уменьшения ключа),
    - **Динамические хэш-таблицы** (амортизированно O(1) для вставки).

---

### Основные методы анализа амортизированной сложности

1. **Метод агрегирования (Aggregate Analysis)**
    
    - Анализируется общая стоимость последовательности операций.
    - Стоимость делится на число операций для вычисления средней стоимости.
    
    **Пример**: Вставка в динамический массив:
    
    - Добавление 1-го элемента: 1 копирование (O(1)).
    - Добавление 2-го элемента: 1 копирование (O(1)).
    - Добавление 3-го элемента: 2 копирования (O(2)).
    - Добавление 5-го элемента: 4 копирования (O(4)).
    - Общая стоимость для 5 вставок: 1 + 1 + 2 + 4 = 8.
    - Средняя стоимость: 8/5=O(1.6)8 / 5 = O(1.6).
2. **Метод учёта (Accounting Method)**
    
    - Каждой операции назначается «счёт» (бюджет), включающий не только её стоимость, но и дополнительный резерв для будущих затрат.
    - Например, каждая вставка в массив «оплачивает» часть будущего копирования.
3. **Метод потенциальных функций (Potential Method)**
    
    - Вводится математическая функция, отражающая «потенциальную энергию» структуры данных.
    - Оценивается, как изменяется эта энергия после каждой операции, и включается в стоимость операции.

---

### Примеры структур данных с разными сложностями

|**Структура данных**|**Операция**|**Худшая сложность**|**Амортизированная сложность**|
|---|---|---|---|
|**Динамический массив**|Вставка в конец|O(n)|O(1)|
|**Хэш-таблица**|Вставка/поиск|O(n)|O(1)|
|**Фибоначчиева куча**|Уменьшение ключа|O(log n)|O(1)|
|**Очередь с двумя стеками**|Удаление из очереди|O(n)|O(1)|

---

### Заключение

Амортизированная сложность — это мощный инструмент для анализа алгоритмов, работающих с последовательностями операций, где некоторые из них могут быть дорогостоящими, но редкими. Она лучше отражает реальную производительность структур данных в долгосрочной перспективе.